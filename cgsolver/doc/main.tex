\documentclass[letterpaper, 11pt]{article}
\usepackage[numbers]{natbib}
\usepackage{thm-restate}
\input{header.tex}
\newcommand*{\newton}{d}
\newcommand*{\hatnewton}{d_{N, T}}

\newcommand{\qup}{q}

\title{Solving convex relaxations for shortest paths in graphs of convex sets}
%\author{Frank Permenter}
\begin{document}
\maketitle
\abstract{ }

\section{Introduction}


\begin{defn}
  For a closed, convex function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ 
  and point $\bar x \in \mathbb{R}$, let  $\tilde f : \mathbb{R}^n \times \mathbb{R}_{+} \rightarrow \mathbb{R} \cup \{\infty\}$
   denote the \emph{perspective} of $f$, defined as
  \begin{align}
\tilde f(x, \varphi) :=  \begin{cases}
  \varphi f(x/\varphi) & \textit{if } \varphi > 0 \\
  \lim_{\varphi \rightarrow 0} \varphi f(\bar x + x/\varphi) & \textit{if } \varphi = 0 \\
  \end{cases}
  \end{align}
  where the limit can be shown to be independent of $\bar x$.
\end{defn}

\begin{defn}
Denote by $\tilde X \subseteq \mathbb{R}^{n} \times \mathbb{R}_{+}$
the \emph{perspective} of $X \subseteq \mathbb{R}^n$, defined as
\[
  \tilde X := \{ (x, \varphi) :   x \in  \varphi X \}.
\]
\end{defn}

\begin{defn}
  Denote by $\epi(f)$ the \emph{epigraph} of $f : \mathbb{R}^n \rightarrow \mathbb{R}$, defined as
\[
  \epi(f) := \{ (x, t) \in \dom(f) \times \mathbb{R} : f(x) \le t \}
\]
\end{defn}


\begin{lem}
  For any closed, convex function $f$, the epigraph $\epi(\tilde f)$
  of the perspective function $\tilde f$ is a convex cone.
  \begin{proof}
    Since $\epi(\tilde f)$ is a convex set we only need to show it is a cone.
    Towards this, suppose $\varphi f(x/\varphi) \le t$. Multiplying both sides by $\alpha > 0$
    shows that $\alpha \varphi f(x/\varphi) \le  \alpha t$.
    Hence, $\alpha \varphi f(\alpha x/\alpha \varphi) \le  \alpha t$,
    showing that $(\alpha x, \alpha \varphi, \alpha t) \in \epi(\tilde f)$
    as desired.
  \end{proof}
\end{lem}

%Up to permutation of the parameters $t$ and $\lambda$,
%the epigraph of the perspective function is the perspective set of the epigraph.
%\begin{lem}
%  Let $X = \epi(f)$ and suppose that $\tilde f(0, 0) = 0$ when $\lambda = 0$.  Then $( (x, t), \lambda ) \in \tilde X$ if and only
%  if $( (x, \lambda), t ) \in \epi(\tilde f)$.
%  \begin{proof}
%    Suppose $\lambda > 0$. Then,
%      $\lambda  f( \frac{1}{\lambda} x) \le t$ if and only if $f( \frac{1}{\lambda} x) \le \frac{t}{\lambda}$.
%      In other words,
%      $ ((x, \lambda) ,t) \in \epi \tilde f$ if and only if
%      $\frac{1}{\lambda}(x, t) \in \epi(f)$.
%      Further,  $\frac{1}{\lambda}(x, t) \in \epi(f)$ if and only if $( (x, t),  \lambda) \in \tilde X$,
%      proving the claim for $\lambda > 0$.
%
%    Consider $\lambda =0$. Then $((x,t), \lambda) \in \tilde X$ implies 
%    that $(x, t) = 0$.   By assumption $\tilde f(x, \lambda) = 0$ when
%    $x =0$ and $\lambda = 0$. Hence, $((x, \lambda), t) \in \epi \tilde f$.
%  \end{proof}
%\end{lem}


\section{Shortest paths in graphs of convex sets}
Given a directed graph $G(V, E)$ the \emph{shortest path} between $s, t \in V$
is a collection of edges $\pi \subseteq E$ connecting $s$ and $t$ that have minimum cumulative weight.
In the classical statement of this problem, the weight
of each edge is a constant function, independent of the path $\pi$. 
A recent generalization considers a richer class of weight functions,
defined by associating each vertex $v \in V$ with a convex set $X_v \subseteq \mathbb{R}^n$ 
and each  edge $e \in E$ with a convex function 
$l_e : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$.
To assign weights, one first selects a set of points $\{x_v \in X_v\}_{v \in V}$
and next evaluates $l_e(x_{u}, x_{v})$ for each edge $e = (u, v) \in E$.
The shortest-path problem seeks a path $\pi$ and point-set $\{x_v \in X_v\}_{v \in V}$ that minimize
the cumulative sum of edge-weights.  An optimal path is called the shortest path in the \emph{graph-of-convex-sets} 
defined by $\{X_v\}_{v\in V}$ and $G$.

As shown in~\cite{}, shortest paths in graphs-of-convex-sets  can be found using mixed-integer convex optimization.
The specific formulation uses \emph{flow variables} $\phi_e \in \mathbb{R}$ and a pair of spatial
variables $(y_e, z_e) \in \mathbb{R}^n \times \mathbb{R}^n$ for each edge.
It minimizes the perspective $\tilde l_e(y, z, \varphi)$ of the edge-cost $l_e$
subject to spatial, flow, and degree constraints.
These constraints in turn are defined by the set of incoming and outgoing edges for each vertex $v \in V$,
denoted $E^{in}_v \subseteq E$ and $E^{out}_v \subseteq E$, respectively.
Letting $s, t \in \mathcal{V}$ denote the desired end-points of the path and $\delta_{uv}$
the Kronecker delta, the formulation is stated as follows:
\begin{subequations}\label{eq:shortest_path}
  \begin{align}
  \minimize_{y_e, z_e, \phi_e, s_v} &\sum_{e\in E}\tilde l_e(y_e, z_e, \varphi_e) \nonumber \\
  \mbox{subject to }  
  & (y_e, \varphi_e) \in \tilde{ X_u},    (z_e, \varphi_e) \in \tilde{ X_v}  &\forall  e=(u, v) \in E \label{eq:spatialConstraint}\\
  & \sum_{e \in E^{in}_v} \varphi_e + \delta_{sv} = \sum_{e \in E^{out}_v} \varphi_e + \delta_{tv} \le 1 &\forall v \in V  \label{eq:conservationFlow}\\
  & \sum_{e \in E^{out}_v} y_e = \sum_{e \in E^{in}_v} z_e &\forall v \in V - \{s, t\}   \label{eq:spatialConservationFlow}\\ 
  & \varphi_e \in \{0, 1\}  & \forall  e \in E  \label{eq:booleanConstraints}
  \end{align}
\end{subequations}
Here,~\eqref{eq:conservationFlow}  imposes both \emph{conservation of flow} and
maximum flow constraints for each node. Combined with~\eqref{eq:booleanConstraints},
it requires that the subset of edges $e$ with $\varphi_e = 1$ form a simple path from $s$ to $t$.
The constraint~\eqref{eq:spatialConservationFlow}
is called \emph{spatial conservation of flow} and, for each node, imposes consistency between
the points associated with incident  edges. Finally,~\eqref{eq:spatialConstraint} combined with~\eqref{eq:booleanConstraints}
says that either $(y_e, z_e) \in X_u \times X_v$  or $(y_e, z_e) = 0$.

\subsection{Convex relaxations}
Our goal is to efficiently solve the convex relaxation of~\eqref{eq:shortest_path}
that arises by replacing the boolean constraints $\varphi_e \in \{0, 1\}$ 
with linear inequalities $0 \le \varphi_e \le 1$. Towards this, we
state the relaxation in a form that is amenable to interior-point methods and sparsity 
analysis.   To begin, associate with each vertex $v$  an inner-product space $\mathcal{V}_v$,
a linear map $A_v : \mathbb{R}^{d} \rightarrow \mathcal{V}_e$, a
convex cone $\coneName_v \subseteq \mathcal{V}_v$ and a vector $b_v \in \mathcal{V}_v$ satisfying
\[
  \tilde X_v = \left\{ (y, \varphi) \in \mathbb{R}^{d+1} : A_v y  + b_v \varphi \in \coneName_v  \right\} \qquad
\]
In this notation, the relaxation takes the form
\begin{align}~\label{cp:coneRelax}
  \begin{aligned}
    \minimize_{y_e, z_e, \phi_e, s_e} & \sum_{e \in E}\tilde l_e(y_e, z_e, \phi_e)\\
    \mbox{subject to } &A_u y_e + b_u \varphi_e \in \coneName_u ,  A_v z_e + b_v \varphi_e \in \coneName_v  & \forall e = (u, v) \in E \\
                      & 1 \ge \varphi \ge 0,  \\
                      & F^{in} \varphi  + s   = f_1, s \ge 0\\
                      & (F^{in} - F^{out}) \varphi  = f_2\\
                      & (F^{in} \otimes I_n)  y - (F^{out} \otimes I_n) z  = 0\\
  \end{aligned}
\end{align}
where $y \in \mathbb{R}^{ n |E|} $ and $z \in \mathbb{R}^{ n |E|}$ denote the concatenation of the $y_e$ and $z_e$
into vectors, and $s \in \mathbb{R}^{|V|}$ is a slack variable for the degree constraint. 
The matrices $F^{in} \in \mathbb{R}^{|V| \times |E|}$ 
and $F^{out} \in \mathbb{R}^{|V| \times |E|}$ are the incidence matrices for incoming and outgoing edges respectively.
Precisely, $[F^{in}]_{ve} = 1$ if edge $e \in E^{in}_{v}$ and is otherwise zero;
similarly, $[F^{out}]_{ve} = 1$ if edge $e \in E^{out}_{v}$ and is otherwise zero.
Finally, $I_n \in \mathbb{R}^{n \times n}$ denotes the identity matrix of order $n$,
and $f_1 \in \mathbb{R}^{|V|}$ and $f_2\in \mathbb{R}^{|V|}$ are constants set according to~\eqref{eq:conservationFlow} .

\subsection{Barrier methods}
The convex relaxation~\eqref{cp:coneRelax} is a special case of the standard-form cone-programming problem
\begin{align}~\label{cp:standardForm}
  \begin{aligned}
    \minimize \;\; &   f(x)\\
    \mbox{subject to } & Ax + b \in \coneName
                       & Fx = g
  \end{aligned}
\end{align}
The barrier method solves~\eqref{cp:standardForm} by
replacing the constraint $Ax + b \in \coneName$ with a penalty function  
$g_K : \inter \coneName \rightarrow \mathbb{R}$ and iteratively solving the optimality conditions of
\begin{align}~\label{qp:barrier}
  \begin{aligned}
    \minimize \;\; &   f(x) + \mu g_{\coneName}(Ax+b) \\
    \mbox{subject to } 
                       & Fx = g
  \end{aligned}
\end{align}
for a decreasing sequence of $\mu > 0$.  
Letting $f_{\mu} = f(x) + \mu  g_K(Ax+b)$, these conditions read
\begin{align}\label{eq:barrier}
  \nabla f_{\mu}(x)+ F^T \lambda = 0, Fx = g,
\end{align}
which in turn can solved using  Newton's method. That is, one can iteratively take
$x \leftarrow x + \Delta x$ where $\Delta x$ solves, for some $\lambda$, the following linear system 
\begin{align}~\label{eq:kktsystem}
  \begin{bmatrix}
    \nabla^2 f_\mu(x)  & F^T  \\
    F  & 0 
  \end{bmatrix}
  \begin{bmatrix}
    \Delta x\\
    \lambda
  \end{bmatrix}
=
  \begin{bmatrix}
    0\\
   g - Fx
  \end{bmatrix}.
\end{align}
We are interested in efficient solution of such systems
when~\eqref{cp:standardForm} is the convex relaxation~\eqref{cp:coneRelax}.
In this case, we'll see that  $f_\mu(x)$ is
always block diagonal and $F$ is sparse when the graph $G$ has few edges. This in turn allows us to efficiently invert
the positive definite Schur complement matrix $F (\nabla^2 f_\mu(x))^{-1} F^T$
using  the conjugate-gradient method.

\subsection{Sparsity of the Newton system}

%The objective of the barrier subproblem  takes the form
%\[
%  f_{\mu}(x) =  \sum_{e \in E}\tilde l_e + \mu \sum_{e \in E}  f^{s}_{e}(y_e, z_e, \varphi_e) + \sum_{v \in v}  f^{s}_{e}(y_e, z_e, \varphi_e) 
%\]
%where $f^s_e$ denotes barrier terms for the spatial inequalities for edge $e$, i.e., for $e = (u, v)$
%we have $f^s_e :=  g_{\coneName_u} (A_u y_e + b_u \varphi_e)  + g_{\coneName_v}(A_v z_e + b_v \varphi_e)$.

Recall that the convex relaxation~\eqref{cp:coneRelax} consists
of a set of edge variables $(y_e , z_e, \phi_e) \in
\mathbb{R}^n \times \mathbb{R}^n \times \mathbb{R}$ for each $e \in E$
and a vector $s \in \mathbb{R}^{|V|}$ for the vertex degree constraints.  By letting $w \in
\mathbb{R}^{(2n+1)|E|}$ denote the concatenation of $(y_e, z_e, \phi_e)_{e \in
E}$, we can express the matrix of the Newton system~\eqref{eq:kktsystem}
as follows:
\begin{align}
  \begin{bmatrix}
    \nabla^2 f_\mu(x)  & F^T  \\
    F  & 0 
  \end{bmatrix} = 
  \bordermatrix{~& w & s & \lambda_1 & \lambda_2  \cr
  &  \diag(G_e)_{e \in E}  & 0  &  F_1^T & F_2^T  \cr
  &  0  & \diag(d_v)_{v \in V}   & I & 0\cr
  &  F_1  & I & 0  & 0 \cr
  &  F_2  & 0 & 0  & 0\cr
  },
\end{align}
where $x = (w, s)$ and $\lambda = (\lambda_1, \lambda_2)$.  Here,  $F_1$ and $F_2$ are sparse matrices, constructed from the node-edge incidence matrices $F^{in}$ and $F^{out}$.
The diagonal matrix $\diag(d_v)_{v \in V}$  consists of barrier terms $d_v$ for the degree constraints at vertex $v$.
Finally, the block-diagonal matrix $\diag(G_e)_{e \in E}$ consists
of blocks $G_e \in \mathbb{R}^{ (2d + 1) \times (2d+1)}$ encoding objective and barrier terms for $(y_e, z_e, \phi_e)_{e \in E}$. For each edge $e=(u,v)$, the block $G_e$ has form:
\[
G_e =
\nabla^2 \tilde l_e(y_e, z_e, \varphi_e) +
  \bordermatrix{~& y_e & z_e & \varphi_e  \cr
&  A_u^T Q^e_u A_u  \cr
&    0          & A_v^T Q^e_v A_v \cr
&   b_u^T Q^e_u A_u    & b_v^T Q^e_v A_v      &  k_e  \cr
 }
\]
where   $Q^e_u$  and $Q^e_p$ arise from the Hessians
of the barrier functions used for spatial constraints,
and $k_{e}$ is constructed from the barrier for $1 \ge \phi_e \ge 0$.
Finally, $\nabla^2 \tilde l_e$ denotes the Hessian of  the edge-cost function $\tilde l_e(y_e, z_e, \varphi_e)$,
when it is twice differentiable.



%\begin{align}
%  \bordermatrix{~& w  & \lambda_1 & \lambda_2  \cr
%  &  G  &   &   \cr
%  &  F_1 & -S^{-1}  & 0  & \cr
%  &  F_2 &0 & 0  & \cr
%  }
%\end{align}
\end{document}

\section{Epigraphs of the objective}

When $\epi \tilde l_e$ is a self-concordant function,
it suffices to take $H_{l}$ equal to its Hessian.
When self-concondance fails, standard practice selects $H_l$
by first describing the \emph{epigraph} of $l_e$ using
cone constraints. For instance, one selects $B_v : \mathbb{R}^{2d+1} \rightarrow \mathcal{V}_v$
satisfying
\[
  \epi \tilde l_e = \left\{ (y,  z, \varphi, t) : B_e(y, z, \varphi, t) \in \coneName_e\right \}
\]
Like the spatial constraints, we associate with $\epi \tilde l_e$
a positive definite matrix
\[
  H_e = B_e^T Q_{l_e} B_e,
\]
where $Q_{l_e}$ depends on $\coneName_{l_e}$
The matrix $H_l$ is the Schur complement of $H_e$ obtained by eliminating the epigraph
parameter $t$.


\section{Primal or dual formulation}

\subsection{Adding cone variables}
\[
  Ax \le b, z \in \coneName, C x + D z = g
\]
Adding cone variables, we obtain
\[
  Ax + s = b, z, s \in \coneName, C x + Dz = g
\]
The dual equality matrix induced by $x$ is
\[
B = \begin{bmatrix}
  A^T & C^T
\end{bmatrix}
\]
The dual inequality matrix induced by $z$  and $s$ is
\[
R = \begin{bmatrix}
  I   & 0\\
     0 & D^T
   \end{bmatrix}
\]
We want $B$ fat and $R$ skinny. 
$B$ is fat if there are more free variables than
equations and inequalities.

Finding Newton direction requires
solving an equality constrained least squares problem with
 and $cols(R^T R) = NumIneq + NumEq$ variables and numFreeVar equations.

\subsection{Adding inequalities}
Converting cone variables to constrained free variables:
\[
  Ax \le b, z \ge 0, C x + D z = g
\]
The augmented inequality matrix has form
\[
  R = \begin{bmatrix}
        A & 0\\
        0 & I
      \end{bmatrix}
\]
It is skinny if $A$ is skinny.

We solve an equality constraint least squares problem with numvar and
and numeq constraints. It is a valid approach when $A$ is skinny.

\subsection{Bean counting}
For node sets defined by $c$ linear inequalities..

We have $E(2*d+1)$ variables.
We have $E(2*c+1)$ linear inequalities.
We have $V*(1+d +1)$ linear equations.


\end{document}
\section{Interior-point methods}
For a class of convex cones $\coneName_v$, the problem~\cite{cp:coneRelax}
is efficiently solved by interior-point methods.  
At each iteration these algorithms apply
the inverse of a \emph{quasi-definite} matrix of the following form:
\begin{align}
\begin{bmatrix}
  G  & F^T \\
  F  & 0   
\end{bmatrix}
\end{align}
where $G$ is positive-definite.   
The matrices $G$ and $F$ can be expressed using the
problem data of~\eq{cp:coneRelax}.
\[
  G = \begin{bmatrix}
    G_{e_1} & 0 & 0
            &G_{e_2}
              & \ddots 
              & \ldots 
                  &G_{e_n}
      \end{bmatrix}
\]
where 
\[
  G_{e_i} = D_e H_e D_e +  \begin{bmatrix}    &  0
                                            0  & 0 +
                           +  \begin{bmatrix} 0 &  0
                                              0 & 0 +
                           \end{bmatrix}
\]




\begin{align}\label{eq:cp}
  A^{T} \lambda = Wx + c, \;\; s = Ax + b, \qquad 
  \lambda \ge 0, s \ge 0,  \;\;s_i \lambda_i = \mu \;\; \forall i \in \{1, 2,\ldots, m\}
\end{align}
for a decreasing sequence of $\mu > 0$.  When $\mu = 0$, these  are precisely the Karush-Kuhn-Tucker
(KKT) optimality conditions for~\eqref{cp:coneRelax}. Hence, 
by gradually reducing $\mu$ to zero, IPMs produce an optimal solution $x$ to~\eqref{cp:coneRelax}
along with an optimal constraint slack $s$ and corresponding vector $\lambda$
of Lagrange multipliers.  IPMs are efficient in practice
and have several high quality implementations~\cite{optimization2012gurobi,
mosek2010mosek}. They are also efficient in theory,
 requiring just $\bigO(\sqrt m)$ iterations to solve the QP~\eqref{cp:coneRelax} to fixed accuracy,
where the per-iteration cost  is the solution of an $n \times n$ linear
system~\cite{achache2006new,  wright1997primal, terlaky2013interior}.  

Success of IPMs requires 
existence and uniqueness of the central path, i.e., of solutions $(x, s, \lambda)$
to~\eqref{eq:cp} for all $\mu > 0$. Using standard arguments (e.g.,~\cite[Theorem~1]{grana2000central}),
this holds by further assuming the QP~\eqref{cp:coneRelax} satisfies
the following conditions.
\begin{ass}\label{ass:main}
  The following conditions hold:
  \begin{itemize}
    \item There exist $x\in\mathbb{R}^n$ and $s\in\mathbb{R}^m$ with $s> 0$ satisfying $s = Ax + b$.
    \item For all $\beta \in \mathbb{R}$, the sublevel set $\{ x \in \mathbb{R}^n : \frac{1}{2}x^{T}Wx + c^{T}x \le \beta, Ax + b \ge 0 \}$ is bounded.
    \item $A^T A + W \succ 0$, i.e.,  $A^T A + W$ is positive definite. 
  \end{itemize}
\end{ass}
\noindent We assume that these conditions hold throughout.

%Under these assumptions, one can prove $O(\sqrt m)$ complexity bounds.
%\noindent These conditions imply that the  \emph{log-barrier function}
%  $f_{\mu}(x) := \frac{1}{2} x^T W x + c^{T}x - \mu \sum^m_{i=1}\log ([Ax+b]_i)$
%   has a unique global minimizer $x_*$ for all $\mu > 0$. Since $\nabla
%f_{\mu}(x_*) = 0$, this in turns implies that $(x_*, s_*, \lambda_*)$ is the unique solution
%of~\eqref{eq:cp} for $s_* := Ax_* + b$ and $\lambda_* := \mu s_*^{-1}$; see,
%e.g.,~\cite[Theorem~1]{grana2000central}.

\subsection{Log-domain interior-point methods}~\label{sec:ipm_template}
The set of nonnegative $(s, \lambda)$ satisfying $s_i \lambda_i = \mu$ for $i \in \{1, 2, \ldots, m\}$
is easily parameterized in the log-domain:
letting $e^v \in \mathbb{R}^m$ denote elementwise exponentiation,
this condition holds if and only if $\lambda = \sqrt \mu e^{v}$ and $s = \sqrt \mu e^{-v}$
  for some $v\in \mathbb{R}^m$.
  This $v$-parametrization of $s$ and $\lambda$ yields the following
   log-domain reformulation of the central-path conditions~\eqref{eq:cp} 
\begin{align}~\label{eq:logcentral}
\sqrt{\mu} A^T   e^v = Wx + c, \;\; \sqrt{\mu} e^{-v}  =  Ax + b
\end{align}

Log-domain IPMs solve these equations using Newton's method. 
Letting $x \circ y$ denote elementwise multiplication of $x, y \in \mathbb{R}^m$, the Newton direction  $\newton(v, \mu)\in\mathbb{R}^m$ and an associated~$x(v,
\mu) \in \mathbb{R}^n$ are defined as follows.
\begin{defn}\label{defn:NewtonDirection}
  For fixed $\mu > 0$ and $v \in \mathbb{R}^m$,
  the \emph{Newton direction} $\newton(v, \mu)$ 
  and associated  $x(v, \mu)$ are the $d \in \mathbb{R}^m$ and $x \in \mathbb{R}^n$
  satisfying
  \begin{align}~\label{eq:cpequations}
    \sqrt \mu   A^T(e^v +  e^v \circ d) =  Wx + c,   \;\;      \sqrt \mu   (e^{-v} -  e^{-v} \circ d)  = Ax + b.
  \end{align}
\end{defn}

In this paper, we study new IPMs that \emph{approximate} $d(v, \mu)$ 
using the conjugate gradient method. 

\subsection{Conjugate gradient method}
The conjugate gradient (CG) method is an iterative algorithm for
solving the linear system $M d = f$ under the assumption that $M \in \mathbb{R}^{m \times m}$ is symmetric and positive definite.
Using the inner-product $\langle x, y \rangle = x^T M y$, each iteration $i$ constructs the orthogonal projection  of the solution  onto the \emph{Krylov subspace}
$\mathcal{K}_i(M, f)$, defined as
\[
\mathcal{K}_i(M, f) = \Span \{ f, Mf, M^2f, \ldots, M^i f \}.
\]
This subspace provably contains a solution for all $i \ge m$. Hence,  the algorithm  always returns
an exact solution (ignoring floating point error) using $N \ge m$ iterations. When $N < m$, the spectrum of $M$ provides rigorous bounds on solution error.


The algorithm is stated as follows.
\begin{align*}
& d_0 :=0,  {r}_0 := {f}, {p}_0 := {r}_0 \\
& k := 0 \\
& \text{while $k < N$} \\
& \qquad \alpha_k := \frac{{r}_k^\mathsf{T} {r}_k}{{p}_k^\mathsf{T} {M p}_k}  \\
& \qquad {d}_{k+1} := {d}_k + \alpha_k {p}_k \\
& \qquad {r}_{k+1} := {r}_k - \alpha_k {M p}_k \\
& \qquad \beta_k := \frac{{r}_{k+1}^\mathsf{T} {r}_{k+1}}{{r}_k^\mathsf{T} {r}_k} \\
& \qquad {p}_{k+1} := {r}_{k+1} + \beta_k {p}_k \\
& \qquad k := k + 1 \\
& \text{end while} \\
& \text{return } {d}_{k+1} \text{ as the result}
\end{align*}
Note that the generation of the vectors $p_0, p_1, \ldots, p_N$ can be interpreted as
Gram-Schmidt orthogonalization applied to $\{ f, Mf, M^2f, \ldots, M^N f \}$. 
Each $d_{k+1}$ in turn is interpreted as the projection of the
solution $d$ onto the subspace  $\mathcal{K}_k$.
Note also that one can incorporate an initial guess $\hat d$ of the solution by replacing $f$ with $f - M \hat d$
at initialization and $d_{N}$ with $d_{N} + \hat d$ at termination.

\section{Approach}
To apply CG to the Newton
system, we first eliminate the variable $x$ to obtain  a linear system $Md = f$ of the desired form
that involves only the Newton direction $d$.
Elimination of $x$ requires the following invertibility assumption.
\begin{ass}\label{ass:Winvertible}
The cost matrix $W$ is invertible.
\end{ass}
A description of the desired linear system follows.
\begin{thm}\label{thm:cgNewtonSystem}
  The Newton direction $d(v, \mu)$ is the unique solution of
  $M(v) d  = f(v, \mu)$, where
  \begin{align*}~\label{eq:cgNewtonSystem}
      M(v) &= I +  \diag(e^v) A W^{-1} A^T \diag(e^v) \\
    f(v, \mu) &= \ones -  \frac{1}{\sqrt\mu} e^v \circ (A  W^{-1}  (\sqrt \mu  A^T(e^v ) -c   )     + b).
  \end{align*}

\begin{proof}
  Using \Cref{ass:Winvertible}, we rearrange the Newton system~\eqref{eq:cpequations}
  to obtain the equivalent linear system
  \[
    x =   W^{-1} ( \sqrt \mu  A^T(e^v +  e^v \circ d)  - c), \qquad d = \ones -  \frac{1}{\sqrt\mu} e^v \circ (Ax + b)
  \]
  Substituting the expression for $x$ into the expression for $d$ gives
  \[
    d = \ones -  \frac{1}{\sqrt\mu} e^v \circ (A  W^{-1}  (\sqrt \mu  A^T(e^v +  e^v \circ d) -c   )     + b)
  \]
  Rearranging proves the claim.
\end{proof}
\end{thm}


\section{Open questions}
We outline a few open questions.

\subsection{Eigenvalue clustering}\label{sec:cluster}

Classical analysis of CG involves spectral properties
of the system matrix $M$. Specifically, one can bound solution error after $N$ iterations
if we can slow that the eigenvalues of $M$ are \emph{clustered} around at most $N$ distinct values.
The following definition provides a quantification of clustering.
The tighter the clustering, the lower  the error bound. 
\begin{defn}
  We say a matrix $M$ is $(N, \delta)$-clustered if 
  there exists a matrix $\hat M$  with at most $N$ distinct eigenvalues 
  satisfying $\|M- \hat M\|_F \le \delta$.
\end{defn}

Recalling that $M$ is a function of $v$ (Theorem~\ref{thm:cgNewtonSystem})
it is natural to ask how $v$ effects  clustering.
\begin{prob}
  Given $N \in \mathbb{Z}$, describe the set of $v$ for which $M(v)$ is $(N, \delta)$-clustered.
\end{prob}

\begin{rem}
By letting $B(v) \in \mathbb{R}^{n \times m}$ denote $W^{-1/2} A^T \diag(e^v)$, we have $M(v) = I + B(v)B(v)^T$.  This shows that the number
of distinct eigenvalues of $M(v)$ equals the number of distinct eigenvalues of $B(v)B(v)^T \in \mathbb{R}^{m \times m}$ and hence of $B(v)^TB(v) \in \mathbb{R}^{n \times n}$. Hence, the number of distinct
eigenvalues of $M(v)$ is upper bounded by $\min(n, m)$.
\end{rem}

\subsection{Distance to Krylov subspaces}
Error analysis for $M d = f$ based only on the clustering of $M$ does not account
for the vector $f$.  To incorporate this vector, we recall
that $N$ iterations of the CG algorithm  projects the solution onto the 
Krylov subspace $\mathcal{K}_N(M, f)$, defined as
\[
  \mathcal{K}_N(M, f) := \Span \{f, Mf, M^2f, \ldots, M^N f \}.
\]
Hence, error can be characterized as the distance of the solution to this subspace.
For fixed $N$ and $v$, it is natural to ask how our selection of $\mu$ effects this distance.
\begin{prob}
  For fixed $(v, N)$, let
   $\mathcal{K}_N(\mu)$ denote the Krylov subspace $\mathcal{K}_N(M(v), f(v, \mu))$
   as a function of $\mu$.
Let $d(\mu)$ denote the solution to $M(v) d = f(v, \mu)$.
As a function of $\mu$, derive an upper-bound of $\dist_{\mathcal{K}_N(\mu)} d(\mu)$.
\end{prob}


\subsection{Preconditioning}
For any invertible $P \in \mathbb{R}^{m \times m}$, 
the vector $d = P^Tz$ solves $Md = f$
when $z$ solves $PMP^Tz = Pf$. 
For well-chosen $P$, the number of CG iterations
needed to solve $PMP^Tz = Pf$ may be far less than those needed
to solve $Md = f$.  Selection of $P$ is called \emph{preconditioning}.
A good preconditioner $P$ has the following properties
\begin{itemize}
  \item The eigenvalues of $PMP^T$ are well-clustered. (\Cref{sec:cluster}).
  \item $Pf$ and $PMP^T$ can be constructed at cost comparable to one CG iteration. 
\end{itemize}
The first property implies that few CG iterations are needed 
to solve $PMP^T z = Pf$ to a given accuracy. The second property  rules out selection
of $P = M^{-1/2}$, since construction of this $P$ is just as hard as solving the original problem.
\begin{prob}
  How to "cheaply" construct a preconditioner $P$ for the Newton system $M(v) = f(v, \mu)$?
\end{prob}


\subsection{Solving multiple systems}

Suppose we want to solve $M(v) d = f(v, \mu)$
for different values of $\mu$.  
\begin{prob}
How can we use the CG iterates generated
on the system $M(v) d = f(v, \mu_0)$
to solve  $M(v) d = f(v, \mu_1)$? 
\end{prob}
\begin{rem}
Section 4.4 of https://www.cs.umd.edu/users/oleary/reprints/j04.pdf discusses a technique. Specifically, it shows
iterations of CG allow one to tridiagonalize $M(v)$. 
\end{rem}



\subsection{Relationship with capacitance matrix methods}

The matrix $M(v)$ has the form of a \emph{capacitance matrix}, i.e., it is of form $I + A B^{-1} C$;
see https://epubs.siam.org/doi/10.1137/1031049.
Evidently these are well studied. 


%Precisely, we provide a concrete algorithm that updates a centered point $(\hat x(\mu_0), \hat v(\mu_0))$   to 
%$(\hat x(\mu_1), \hat v(\mu_1))$ using at most $C \sqrt m \log \frac{\mu_0}{\mu_1}$ Newton iterations,
%where $(\hat x(\mu), \hat v(\mu))$ denotes the solution of~\eqref{eq:logcentral}
%as a function of $\mu$ and $C$ is a fixed constant.  


%It also has a few conceptual advantages.
%%Whereas these algorithms preserve the affine constraints $A^T \lambda = Wx + c$
%%and $s=Ax+b$ at each iteration, this preserves complementarity slackness.
%First, it operates on a reduced number of variables, i.e., $(x, v)$ instead of $(x, s, \lambda)$.
%Second, non-negativity of $(s, \lambda) = (\sqrt\mu e^{-v},\sqrt\mu e^{v})$ is automatic, 
%whereas primal-dual IPMs must maintain $(s, \lambda) \ge 0$ using line search. 



%globally convergences using
%a simple step-size rule.
%by leveraging properties of the exponential function, we can easily establish a globally
%convergent step-size rule and local quadratic convergence.  
%Finally, an $\bigO(\sqrt m)$ bound on total Newton iterations, which is typical
%for primal-dual IPMs, can be obtained using a simple update rule for $\mu$.

%\noindent These conditions imply that the  \emph{log-barrier function}
%  $f_{\mu}(x) := \frac{1}{2} x^T W x + c^{T}x - \mu \sum^m_{i=1}\log ([Ax+b]_i)$
%   has a unique global minimizer $x_*$ for all $\mu > 0$. Since $\nabla
%f_{\mu}(x_*) = 0$, this in turns implies that $(x_*, s_*, \lambda_*)$ is the unique solution
%of~\eqref{eq:cp} for $s_* := Ax_* + b$ and $\lambda_* := \mu s_*^{-1}$; see,
%e.g.,~\cite[Theorem~1]{grana2000central}.
%
%\fbox{TODO}
%\begin{lem}
%The Hessian of $f_{\mu}$
%is
%  \[
%  A^T Q A + W
%  \]
%  Let $S(x) = (Ax+b)^{-1}$
%The gradient of $f_{\mu}$ is
%  \[
%    Wx + c - A^T S(x) 
%  \]
%  and the Hessian
%  \[
%    W + A^T S(x) S(x) A
%  \]
%  This shows that it is strictly convex if $W + A^T A > 0$
%  Otherwise, we would have $x$ satisfying
%  \[
%    Wx = S(x) A x = 0.
%  \]
%  which implies
%  \[
%  Ax = 0
%  \]
%  which implies $(W + A^T A) x = 0$, a contradiction.
%
%  Hence, if there is a critical point it must be unique.
%  But since the sublevel set is bounded, the infimum is obtained (why?).
%
%
%  Let $v_1$ and $v_2$ be two solutions to the central path equations.
%  Then
%  \[
%  e^{-v_1} \ne Ax_1 + b \ne  e^{-v_2} \ne Ax_2 + b
%  \]
%  But this implies that $x_2 \ne x_1$, contradicting
%  uniqueness of $x$.
%  
%
%
%
%
%\end{lem}
\end{document}
\subsection{Prior work}

The literature on quadratic programming is vast and we will not attempt to cite
it completely. We do note that  IPMs with $\bigO(\sqrt
m)$ iteration bounds include~\cite{monteiro1989interiorQuad, goldfarb1990n, goldfarb1993n, kojima1991sqrt}.  
One can also obtain a $\bigO(\sqrt m)$ bound by invoking the \emph{self-concordance} of a suitable
\emph{barrier function}; see~\cite{nesterov1994interior}.
For linear objectives ($W=0$), our algorithms are special cases of 
\emph{geodesic interior-point methods}~\cite{permenter2020geodesic},  recent
techniques for minimizing a \emph{linear} function  subject to \emph{symmetric
cone} inequalities. Indeed, our main analysis task is showing that key
convergence results of~\cite{permenter2020geodesic} still hold 
when a quadratic objective term $x^{T}Wx$ is included.

Linear updates of the log parameter $v$ 
are of course multiplicative updates
of $s = \sqrt\mu e^{-v}$  and $\lambda = \sqrt\mu e^{v}$.
Algorithms based on multiplicative updates
have been developed for restricted families of QPs, e.g.,
linear programs $(W = 0)$, nonnegative least-squares $(A = I, b = 0)$,
and model-predictive control; see, e.g.,~\cite{arora2012multiplicative, sha2007multiplicative, di2013multiplicative}. 
We emphasize that in each of these algorithms, 
the updates are distinct from ours and are designed in different ways. In
particular, they are only applied to one variable, $\lambda$ or $s$, and are
not based on the log-transformation~\eqref{eq:logcentral} of the central-path
conditions.


%It turns
%out that the algorithms presented here also 
%generalize to symmetric cone inequalities.  We will sketch these
%generalizations, but focus our main exposition on linear constraints $Ax + b
%\ge 0$ for simplicity.  


%Jsketch how they extend to quadratic objectives for general symmetric cone
%Jinequalities.  We focus our main exposition  on linear constraints $Ax + b \ge
%J0$ for simplicity.

%As a main contribution, we analyze Newton's method applied to~\eqref{eq:logcentral},
%establishing a globally-convergent step-size rule and a local region of 
%quadratic convergence. We also provide a simple rule for decreasing
%$\mu$ that yields an $\bigO(\sqrt m)$ bound on total Newton iterations, which
%is typical of interior-point methods. 
%Finally, we give an alternative rule
%that more aggressively updates $\mu$ and illustrate
%its practical performance. 
%\fbox{expand}


%which corresponds to $x^*, s = Ax^* + b, 
%\lambda  = \mu \nabla \log(Ax^* + b)$ on the central-path.


\subsection{Outline and contributions}
This paper is organized as follows. \Cref{sec:Newton} analyzes the application
of Newton's method to the log-domain central-path equations~\eqref{eq:logcentral},
establishing a globally-convergent step-size rule and a local region of
quadratic convergence.  Building on this analysis, Section~\ref{sec:alg}
provides two algorithms  for solving the QP~\eqref{cp:coneRelax}
based on two different $\mu$-update rules. The first is a short-step
algorithm that reduces $\mu$ at a fixed rate and  terminates after at most
$\bigO(\sqrt m)$ Newton iterations.  The second is a long-step algorithm that
employs more aggressive $\mu$-updates via line-search.
Section~\ref{sec:comp} illustrates practical performance
of the latter algorithm.
%as we illustrate via computational
%experiments (\Cref{sec:comp}).~\Cref{sec:sym} discusses the extension
%to symmetric cone inequalities.

%\subsection{Prior work}
%
%Gives hard LPs with bad central paths:
%https://arxiv.org/pdf/1708.01544.pdf
%
%Updates of iterates using quadratic approximations of 
%the central path~\cite{yang2011polynomial}.

\section{Newton's method}~\label{sec:Newton}
Applying Newton's method to the log-domain central-path
equations~\eqref{eq:logcentral} proceeds by Taylor-approximating the
exponential functions $e^v$ and $e^{-v}$.
Letting $x \circ y$ denote elementwise multiplication of $x, y \in \mathbb{R}^m$,
these approximations take the form
\begin{align*}
  \begin{aligned}
    e^{v + d} &\approx  e^v +  e^v \circ d, \\
  \end{aligned}
  \qquad
  \begin{aligned}
    e^{-(v + d)} &\approx  e^{-v} - e^{-v}\circ  d.
\end{aligned}
\end{align*}
The Newton direction  $\newton(v, \mu)\in\mathbb{R}^m$ and an associated~$x(v,
\mu) \in \mathbb{R}^n$ are then defined by substituting
these approximations into~\eqref{eq:logcentral}.
\begin{defn}\label{defn:NewtonDirection}
  For fixed $\mu > 0$ and $v \in \mathbb{R}^m$,
  the \emph{Newton direction} $\newton(v, \mu)$ 
  and associated  $x(v, \mu)$ are the $d \in \mathbb{R}^m$ and $x \in \mathbb{R}^n$
  satisfying
  \begin{align*}
    \sqrt \mu   A^T(e^v +  e^v \circ d) =  Wx + c,   \;\;      \sqrt \mu   (e^{-v} -  e^{-v} \circ d)  = Ax + b.
  \end{align*}
\end{defn}
Note that when $d(v, \mu) = 0$,
we obtain a point  $(x, s, \lambda)$ on the central-path
by taking $x = x(v, \mu)$,  $s =\sqrt\mu e^{-v}$, and $\lambda= \sqrt\mu e^v$.
Further, $x(v, \mu)$ is a ``good'' approximate
solution of the QP~\eqref{cp:coneRelax} when $\mu$ and $\|d(v, \mu)\|$ are sufficiently
small. (Exact error bounds will be given in Section~\ref{sec:longstep}.)
It remains to prove that $\newton(v, \mu)$  and $x(v, \mu)$ are well-defined
for all $\mu >0$ and $v \in \mathbb{R}^m$.
For this, we next show that $\newton(v, \mu)$ is a function of $x(v, \mu)$
and that $x(v, \mu)$ is the unique solution of a consistent linear system.
In particular, we show that this linear system is of the form $Sx = f$ for $S \succ 0$, i.e.,
for $S$ symmetric and positive definite. 
\begin{thm}
  For all $v \in \mathbb{R}^m$ and $\mu > 0$, the Newton direction $\newton(v, \mu)$ and point $x(v, \mu)$ satisfy
  \[
  \newton = \ones -  \frac{1}{\sqrt\mu} e^v \circ (Ax + b),
  \]
where $\ones \in \mathbb{R}^m$ denotes the vector of all ones. 
  Moreover, $x(v, \mu)$ is the unique solution of
  \[ 
  (A^T Q(v) A +  W)x = 2\sqrt \mu  A^{T} e^v  -(c+ A^{T}Q(v)b),
  \]
where $Q(v) \in \mathbb{R}^{m \times m}$ is the diagonal matrix with $[Q(v)]_{ii} = e^{2v_i}$. 
Further, $A^T Q(v) A +  W \succ 0$.

  \begin{proof}
  Rearranging $\sqrt \mu   (e^{-v} -  e^{-v} \circ d)  = Ax + b$, we conclude that
\begin{align*}
  d &=  \ones -  \frac{1}{\sqrt\mu} e^v \circ (Ax + b).
\end{align*}
Substituting into $\sqrt \mu   A^T(e^v +  e^v \circ d) =  Wx + c$ yields
\begin{align*}
  Wx + c &= \sqrt \mu A^{T} e^v \circ (\ones+d) \\
   &=\sqrt \mu  A^{T} e^v \circ ( \ones + \ones - \frac{1}{\sqrt\mu} e^v \circ ( Ax + b)).
\end{align*}
Rearranging and using $Q(v)Ax = e^v \circ (e^v \circ Ax)$ shows that
\[
  (A^T Q(v)  A +  W) x  =     2\sqrt \mu  A^{T} e^v  -(c+ A^{T}Q(v)b).
\]
 Uniqueness of $x$ follows because  $A^T Q(v)  A +  W$ is strictly
    positive definite under our assumption that $A^{T}A + W$ is strictly positive definite (\Cref{ass:main}).
    To see this, suppose that $(A^T Q(v)  A +  W) z = 0$ for nonzero $z$.
    Then, $Wz = 0$ and  $A^T Q(v)  A  z = 0$.  But this implies that $Q(v)^{1/2}  A  z = 0$, which,
    in turn means that $Az = 0$ since $Q(v)^{1/2}$ is invertible. We conclude that $(A^T A + W)z = 0$,
    a contradiction.
  \end{proof}
\end{thm}


The remainder of this section studies convergence of the Newton iterations
  $v_{i+1} = v_i + \frac{1}{\alpha_i} d(v_i, \mu)$
under a simple step-size rule  for choosing  $\alpha_i \in \mathbb{R}$.
We will show global convergence to \emph{centered points}.
\begin{defn}[Centered points]
  For $\mu > 0$, the \emph{centered point} $\hat v(\mu)$ is the $v\in\mathbb{R}^m$ that, for some $x\in\mathbb{R}^n$, solves the log-domain
central-path equations~\eqref{eq:logcentral}.
\end{defn}
\noindent Following~\cite{permenter2020geodesic}, we will measure the distance
of an iterate $v_i$ to $\hat v(\mu)$ using \emph{divergence}.
\begin{defn}[Divergence~\cite{permenter2020geodesic}]
  The \emph{divergence} $h(u, v)$ of $(u, v) \in \mathbb{R}^m \times \mathbb{R}^m$
  is
\[
    h(u, v) := \langle e^{u}, e^{-v}\rangle  + \langle e^{-u}, e^{v}\rangle - 2m.
\]
  For fixed $\mu > 0$, the function $h_{\mu} : \mathbb{R}^m \rightarrow \mathbb{R}$ 
  denotes the map $v \mapsto h(\hat v(\mu), v)$. 
\end{defn}
\noindent While divergence is \emph{not} a metric,
it does have a set of properties useful for convergence analysis.
\begin{lem}\label{lem:divprop}
  The following properties hold for all $u, v \in \mathbb{R}^m$ and $\mu > 0$.
  \begin{enumerate}[label= (\alph*)]
    \item  $h(u, v) = h(v, u)$ and $h(u, v) \ge 0$.
    \item\label{item:divposdef}  $h(u, v) = 0$ if and only if $u = v$. In particular,
      $h(u, v) = -2m + \sum^m_{i=1} 2\cosh(v_i - u_i)$.
    \item\label{item:divstrongconvex}$h_{\mu} : \mathbb{R}^m \rightarrow \mathbb{R}$ is strongly convex. In particular, $\frac{1}{2}\nabla^2 h_{\mu}(v) \succeq I$.
  \end{enumerate}
\end{lem}
%Note both of these properties following trivially by observing that
%\[
%  h(u, v) = 2 \sum^m_{i=1}( \cosh(u_i-v_i) - 1)
%\]
\noindent Leveraging these properties, Section~\ref{sec:stepsize} 
shows that $h_{\mu}(v_{i}) < h_{\mu}(v_{i-1})$ for all iterations $i$ under a simple step-size rule.
Building on this, Section~\ref{sec:globalconv}
shows that the sequence $v_0, v_1, v_2,\ldots$ converges to the centered point $\hat v(\mu)$ 
from an arbitrary initial $v_0 \in \mathbb{R}^m$.
Finally, Section~\ref{sec:quadconv} shows
quadratic convergence  when $h_{\mu}(v_0) \le \frac{1}{2}$.
As we will point out, some statements generalize previous
results for linear optimization~\cite{permenter2020geodesic} 
to the quadratic program~\eqref{cp:coneRelax}.


%The remainder of this section studies convergence of Newton iterations
%to $\hat v(\mu)$.  In particular, it establishes
%for all $v_0 \in \mathbb{R}^m$ and $\mu > 0$ that the sequence
%\[
%  v_{i+1} = v_i + \frac{1}{\alpha_i} d(v_i, \mu)
%\]
%convergences  for $\alpha_i = \max(1, \frac{1}{2}\|d(v_i, \mu)\|^2_{\infty})$.
%Quadratic convergence is also demonstrated on the $\frac{1}{2}$-sublevel
%set of \emph{divergence} $h : \algName \times \algName \rightarrow \mathbb{R}$,
%defined as
%\[
%    h(u, v) := \langle e^{u}, e^{-v}\rangle  + \langle e^{-u}, e^{v}\rangle - 2m.
%\]
%In passing, we observe that  $h(u, v) = 2 \sum^m_{i=1} ( \cosh(v_i - u_i) - 1)$,
%where $\cosh : \mathbb{R} \rightarrow \mathbb{R}$
%denotes hyperbolic cosine $t \mapsto \frac{1}{2}(e^t + e^{-t})$.
%From the power-series expansion of $\cosh(t)$, it follows
%that $h(u, v) \ge \|u-v\|^2$.
%Divergence is \emph{not} a metric, as the tri
%Though it is symmetric, i.e., $h(u, v) = h(v, u)$,  and it vanishes, i.e., $h(u, v)=0$, if
%and only if $v = u$.  
%It, however, is \emph{not} a metric, as the
%triangle inequality can fail.


\subsection{Step-size rule}~\label{sec:stepsize}
Our step-size rule arises from bounds on the directional
derivatives of divergence $h_{\mu}(v)$.
Towards stating them, fix $v \in \algName$ and  $\mu > 0$
and for brevity let $d\in\mathbb{R}^m$ denote the Newton direction $\newton(v, \mu)$.
Assume that $d \ne 0$ or, equivalently, that $v \ne \hat v(\mu)$.
Finally, let $f : \mathbb{R} \rightarrow \mathbb{R}$ 
denote the restriction of $h_{\mu}(v)$ to the line induced by $v$ and $d$, i.e.,
\[
  f(t) := h_{\mu}(v + t d).
\]
The next lemma provides bounds on $f'(0)$ and $f''(t)$ and
generalizes~\cite[Lemma
3.4]{permenter2020geodesic}~and~\cite[Lemma 3.6]{permenter2020geodesic}.  
\begin{lem}\label{lem:NewtonDirDerivs}
The following statements hold.
  \begin{itemize}
    \item $f{'}(0) \le  -(f(0) + \|d\|^2)$.
    \item $f^{''}(t) \le \|d\|^{2}_{\infty} f(t) + 2 \|d\|^2$.
    \item For all intervals $[a, b]\subset \mathbb{R}$, we have $\sup_{\zeta \in [a,b]} f''(\zeta) \le  \max_{\zeta \in \{a, b\}} (\|d\|^{2}_{\infty} f(\zeta) + 2 \|d\|^2)$.
  \end{itemize}
  \begin{proof}
    For brevity, let $w = e^v$, $\hat w = e^{\hat v(\mu)}$ and $k = \sqrt \mu$.
Letting $z^{-1}$ denote elementwise inversion,
    define $p := \langle w^{-1} \circ (\ones- d) -  \hat w^{-1},   w \circ(\ones+d)- \hat w \rangle$.
Expanding, we conclude that
\begin{align*}
  p &= \langle (\ones -d), (\ones +d) \rangle -\langle w^{-1} \circ \hat w, (\ones -d) \rangle -\langle w \circ\hat w^{-1}, (\ones +d) \rangle
+m \\
  &=m - \|d\|^2  -\langle w^{-1} \circ \hat w, (\ones -d) \rangle -\langle w \circ \hat w^{-1}, (\ones +d) \rangle + m\\
  &= -( \langle  \hat w, w^{-1} \rangle + \langle  \hat w^{-1}, w \rangle - 2m)   - \|d\|^2  -
  \langle w \circ \hat w^{-1} - w^{-1} \circ \hat w, d \rangle
\\
  &= -f(0) - \|d\|^2 - f'(0)  .
\end{align*}
We now show that $p \ge 0$ for the Newton direction $d$, which will prove the first statement.  Since $\hat w$ solves~\eqref{eq:cp}, we have for some $\hat x$, that
  $b = k \hat w^{-1} - A\hat x$ and   $c = kA^{T}\hat w  - W\hat x$.
Substituting these expressions for $b$ and $c$ into the definition of $d$, we have,
for some $x$, that
    \[
      k w^{-1} \circ (\ones- d) - k \hat w^{-1}   = A(x - \hat x) , \;\;
      k A^T (w \circ (\ones+ d) -  \hat w) = W(x - \hat x).
    \]
Using the first equation, we  conclude that $p = \langle \frac{1}{k}A(x-\hat
    x),  w \circ(\ones+d)- \hat w  \rangle$.  Combining with the second yields
    $p  = \langle \frac{1}{k}(x - \hat x), \frac{1}{k} W(x-\hat x) \rangle \ge
    0$, which proves the first statement.  Proof of the second statement is identical to~\cite[Lemma
    3.4.c]{permenter2020geodesic}.  The third statement follows from the second
    and convexity of $f(t)$.
  \end{proof}
\end{lem}
Combining this lemma with the inequality 
\begin{align}~\label{eq:taylorup}
f(t) \le f(0) + f'(0) t + \frac{1}{2} \sup_{\zeta \in [0, t]} f''(\zeta) t^2
\end{align}
yields a piecewise step-size rule for selecting $t$ such that $f(t) < f(0)$. This rule
is parameterized by $0 < \beta < 1$ which, along with 
$\|d\|_{\infty}^2$, controls the transition from full to damped steps. 
%using Lemma~\ref{lem:NewtonDirDerivs}.
\begin{thm}~\label{thm:newtonstep}
  For $\beta \in (0, 1)$,  let $\alpha  = \max(1, \frac{1}{2\beta} \|d\|_{\infty}^2)$. 
  The following statements hold.
  \begin{enumerate}[label= (\alph*)]
    \item~\label{item:thm:newton:strict}$f(\frac{1}{\alpha}) < f(0)$
    \item~\label{item:thm:newton:full}If $\alpha=1$, then $f(1) \le \frac{1}{2} \|d\|^2_{\infty} f(0) \le \beta f(0)$.
  \end{enumerate} 
  \begin{proof}
    Let $\hat t \ge 0$  denote the smallest $t$ for which $f(t) = f(0)$. By
    strong convexity (\Cref{lem:divprop}), we have that $f(t) < f(0)$ for all $t \in (0, \hat t)$
    since $d$ is a descent direction (\Cref{lem:NewtonDirDerivs}).
    Towards bounding $\hat t$, we first note that the combination of~\eqref{eq:taylorup} with Lemma~\ref{lem:NewtonDirDerivs}
    implies that for all $t$,
    \begin{align}\label{eq:newtonProof}
      f(t) \le f(0) - t(f(0) + \|d\|^2) + \frac{1}{2}(\|d\|^2_{\infty} \max(f(0), f(t)) + 2\|d\|^2) t^2.
    \end{align}
    Substituting $t = \hat t$ and using $f(0) = f(\hat t)$, we conclude that
    \[
      \hat t (\frac{\|d\|^2_{\infty}}{2} f(0) + \|d\|^2 ) \ge f(0) + \|d\|^2.
    \]
    Hence, $t < \hat t$ if $t (\frac{\|d\|^2_{\infty}}{2} f(0) + \|d\|^2 ) < f(0) + \|d\|^2$,
    which holds if $t  = \min(1, \frac{2\beta}{\|d\|_{\infty}^2})$,
    proving~\cref{item:thm:newton:strict}.
    % To see this, suppose $t = 1$. Then  \frac{2\beta}{\|d\|_{\infty}^2 > 1,
    % which shows that \|d\|_{\infty}^2  < 2 beta < 2. Hence \|d\|_{\infty}^2/2 < 1.
    % On the other hand, if t = 2 beta / \|d\|_{\infty}^2 \le 1,
    % we have \beta f(0) + 2 beta / \|d\|_{\infty}^2  \|d\|^2 < f(0) + \|d\|^2.
    \Cref{item:thm:newton:full} follows by substituting $t = 1$ 
    and $\max(f(0), f(1))  = f(0)$  into~\eqref{eq:newtonProof}.
  \end{proof}
\end{thm}
%\noindent For arbitrary $v_0 \in \mathbb{R}^m$, this theorem immediately convergence of $h( v_i, \hat v(\mu))$ 
%for Newton iterations $v_{i+1} = v_{i} + \frac{1}{\alpha_i} d(v_i, \mu)$ with step-size rule $\alpha_i =\max\{1, \frac{1}{2\beta}\|d(v_i, \mu)\|^2_{\infty}\}$
%for $0 < \beta < 1$.
%\begin{thm}
%  $f(t) \le f(0)$ if $0 \le t \le 1$ and $t\le \frac{2}{\|d\|^2_{\infty}}$.
%  Further, $f(t) < f(0)$ if in addition $t < 1$ or $t < \frac{2}{\|d\|^2_{\infty}}$.
%
%  \begin{proof}
%    Further, $f(t) < f(0)$ if this inequality is strict by strict convexity of $f$.
%  \end{proof}
%\end{thm}

\subsection{Global convergence}~\label{sec:globalconv}
Newton iterations strictly decrease the divergence $h_{\mu}(v)$ under the
step-size rule of~\Cref{thm:newtonstep}.  Combined with  the strong convexity
of $h_{\mu}$, this implies  convergence to the centered point $\hat v(\mu)$.
\begin{thm}\label{thm:globalconv}
  Fix $0 < \beta < 1$ and $\mu > 0$.  For all $v_0 \in \mathbb{R}^m$, 
the Newton iterations $v_{i+1} = v_{i} + \frac{1}{\alpha_i} d(v_i, \mu)$ with step-size rule $\alpha_i =\max\{1, \frac{1}{2\beta}\|d(v_i, \mu)\|^2_{\infty}\}$
  converge to the centered point $\hat v(\mu)$. 
  \begin{proof}
    By choice of $\alpha_i$ and
    \Cref{thm:newtonstep}-\ref{item:thm:newton:strict}, we have that
    $h_{\mu}(v_i)$ strictly decreases.
    In addition, $h_{\mu}(v_i) \ge 0$ for all $v_i$; hence, it converges to some nonnegative number $\delta$.
    We will show that $\delta = 0$, which implies that $v_i$ converges to $\hat v(\mu)$ by \Cref{lem:divprop}-\ref{item:divposdef}.
    To begin, note that all iterations $v_i$ are contained in the set
      $\Omega := \{ v \in \mathbb{R}^m : \delta \le h_{\mu}(v) \le h_{\mu}(v_0) \}$.
    But $\Omega$ is compact since $h_{\mu}$ is strongly convex (\Cref{lem:divprop}).
    Letting $\alpha(v) := \max\{1, \frac{1}{2\beta}\|d(v, \mu)\|^2_{\infty}\}$,
    we conclude that the continuous function
      $D(v) :=  h_\mu(v) - h_\mu(v + \frac{1}{\alpha(v)} d(v, \mu))$
    obtains its infimum $D^*\in\mathbb{R}$ on $\Omega$.  But if $\delta > 0$,
    then $D^* > 0$, which implies that $h_{\mu}(v_m) \le h_{\mu}(v_0) - m D^* < 0$
    for all $m >  h_{\mu}(v_0)/D^{*}$, a contradiction since $h_{\mu}(v_m)  \ge
    0$.  Hence, $\delta = 0$. 
  \end{proof}
\end{thm}


\subsection{Local quadratic convergence}~\label{sec:quadconv}
 \Cref{thm:newtonstep} states that  a full Newton-step $v_{i+1} = v_i + d(v_i, \mu)$
 decreases the divergence $h_{\mu}(v_i)$ by a factor of at least
 $\frac{1}{2}\|d(v_i, \mu)\|^2_{\infty}$.  The next lemma, which generalizes~\cite[Corollary 3.1]{permenter2020geodesic},
 shows that we can also upper-bound $\|d(v_i, \mu)\|^2$, and hence $\|d(v_i, \mu)\|^2_{\infty}$,
 using $h_\mu(v_i)$.  
\begin{lem}~\label{lem:divBound}
  For all $v \in \mathbb{R}^m$ and $\mu > 0$, it holds that $  \|d(v, \mu) \|^2
  \le h_{\mu}(v) (1+\|d( v, \mu) \|)$. 
  \begin{proof}
    For brevity, let $d$ denote $d( v, \mu)$ and let $a = e^{v-\hat v(\mu)}$ and $g = a - a^{-1}$.
    As in Section~\ref{sec:globalconv}, let $f(t) = h_{\mu}(v + t d)$ such that $f(0) = h_{\mu}(v)$.
    Observing that $g$ is the gradient of $h_{\mu}(v)$ with respect to $v$, we have,
    by Lemma~\ref{lem:NewtonDirDerivs} and Cauchy-Schwartz, that
    \[
      f(0) + \|d\|^2  \le |f'(0)| = |\langle g, d \rangle| \le \|g\| \|d\|.
    \]
    We also have that $\|g\|^2 \le f(0)^2 + 4f(0)$ given that
\[
  f(0) = \|a+a^{-1} - 2\ones \|_1  \ge \|a+a^{-1} - 2\ones \|_2 =  \sqrt{\|a - a^{-1} \|^2 - 4 \langle \ones, a + a^{-1} - 2\ones\rangle} =  \sqrt{ \|g\|^2 - 4 f(0)}.
\]
    We conclude that $f(0) + \|d\|^2 \le \|d\| \sqrt{f(0)^2 + 4f(0)}$.
    Squaring each side and rearranging yields
    \begin{align*}
      0 \le \|d\|^2 (f(0)^2 + 4f(0)) - (f(0) + \|d\|^2)^2 = (-\|d\|^2 + f(0)(1+\|d\|)) (\|d\|^2 + f(0)(\|d\|-1)).
    \end{align*}
    This shows that if $f(0)(1+\|d\|) < \|d\|^2$, then $\|d\|^2  \le  f(0)(1-\|d\|)$,
    which in turn implies that
    \[
      f(0)(1+\|d\|)  < f(0)(1-\|d\|),
    \]
    which is impossible. Hence, $f(0)(1+\|d\|) \ge \|d\|^2$, as desired.

   % Using the AGM inequality,
   % \[
   %   f(0) + \|d\|^2 \le \frac{1}{2}(\|d\|^2 + f(0)^2 + 4f(0)) 
   % \]

   % \[
   %   f(0) + \|d\|^2 \le \frac{1}{2}(\|d\|^2 + f(0)^2 + 4f(0)) 
   % \]

   % \[
   %   \frac{1}{2} \|d\|^2 \le  \frac{1}{2}( f(0)^2 + 2f(0)) \le \frac{3}{2} f 
   % \]


   % Equivalently,
   % \[
   %   \sqrt{f(0)^2 + 4f(0)} - f(0) \le 2 \|d\| \le   \sqrt{f(0)^2 + 4f(0)} + f(0)
   % \]
    %Squaring each side yields
    %\[
    %  0 \le  (h^2 + 4h) \|d\|^2  - {(h+\|d\|^2)}^2   = (\|d\|^2-1) (h - \frac{\|d\|^2}{\|d\|+1})(h + \frac{\|d\|^2}{\|d\|-1}),
    %\]
    %which proves the claim.
%  if $\|d\|^2-1 > 0$, then
%  \[
%   h - \frac{\|d\|^2}{\|d\|+1} \ge 0
%  \]
%  on the other hand, if $\|d\|^2- 1 < 0$
%  \[
%(h - \frac{\|d\|^2}{\|d\|+1})(h + \frac{\|d\|^2}{\|d\|-1}) < 0
%  \]
%
%  hence, either
%  \[
%  h - \frac{\|d\|^2}{\|d\|+1} > 0, \qquad h + \frac{\|d\|^2}{\|d\|-1} < 0
%  \]
%or
%  \[
%  h - \frac{\|d\|^2}{\|d\|+1} < 0, \qquad h + \frac{\|d\|^2}{\|d\|-1} > 0
%  \]
%but these latter inequalities are inequivalent to
%  \[
%  h(\|d\|+1) <  \|d\|^2 < h(1-\|d\|),
%  \]
%which contradict each other.
  \end{proof}
\end{lem}
\noindent Combining this with \Cref{thm:newtonstep}
yields the following quadratic convergence result,
which generalizes~\cite[Theorem 3.4]{permenter2020geodesic}.
\begin{thm}~\label{thm:newtonConv}
  For $\mu >0$ and $v_0 \in \mathbb{R}^m$, 
  let $v_{i+1} =
  v_i + \newton(v_i, \mu)$.  If $h_{\mu}(v_0) \le \beta \le \frac{1}{2}$, then
    $h_{\mu}(v_i) \le   \beta^{2^i}$.
  \begin{proof}
    Let $h_i = h_{\mu}(v_i)$ and $d_i =\newton(v_i, \mu)$.  
    Make the inductive hypothesis that $h_i \le \frac{1}{2}$. Then Lemma~\ref{lem:divBound}
    implies that $\|d_i \|  \le 1$. Hence,
  \[
    h_{i+1} \le \frac{1}{2} h_i \|d_i\|_{\infty}^2  \le   \frac{1}{2} h_i \|d_i\|^2 \le  \frac{1}{2} h_i ( \|d_i\|+1) h_i,
  \]
    where the first inequality is \Cref{thm:newtonstep}~\ref{item:thm:newton:full} and the last
    is~\Cref{lem:divBound}. Since $\|d_i\|  \le 1$, we further conclude
    that $h_{i+1} \le h^2_i$, and that $h_{i+1} < \frac{1}{2}$. By induction,  
    $h_{i+1} \le h^2_i$ must hold for all $i$, which implies that  $h_i \le (h_0)^{2^i}$ as claimed.
  \end{proof}
\end{thm}
Using the results of this section, we can now concretely instantiate
the template log-domain interior-point method (\Cref{sec:ipm_template}).
The next section will state two algorithms.
\section{Algorithms}~\label{sec:alg}
\begin{figure}
  \centering
  \begin{minipage}[t]{.45\linewidth}~\begin{algorithm}[H]
  \SetAlgoLined\DontPrintSemicolon{}
  \SetKwFunction{algo}{shortstep}\SetKwFunction{proc}{Center}
  \SetKwProg{myalg}{Procedure}{}{}
  \myalg{\algo{$v_0, \mu_0, \mu_f$}}{
  \vspace{.1cm}
  $v \leftarrow v_0$, $\mu \leftarrow \mu_0$\\
  \While{$\mu  > \mu_f$} 
  {
    $\mu \leftarrow \frac{1}{k}\mu $ \\
    \For{$i = 1, 2, \ldots, N$}
    {
      $v \leftarrow v  + \newton(v,  \mu)$ \\
    }
 }
    \Return{$(v, \mu)$}  \\
  } 
  \end{algorithm}\end{minipage}~\begin{minipage}[t]{.65\linewidth}~\begin{algorithm}[H]
\vspace{-.35cm}
  \SetAlgoLined\DontPrintSemicolon{}
  \SetKwFunction{algo}{longstep}\SetKwFunction{proc}{Center}
  \SetKwProg{myalg}{Procedure}{}{}
  \myalg{\algo{$v_0, \mu_0, \mu_f$}}{
  \vspace{.1cm}
  $v \leftarrow v_0$, $\mu \leftarrow \mu_0$\\
    \While{$\mu  > \mu_f$ {\bf or} $\|d(v, \mu)\|_{\infty} > 1$} 
  {
    $\mu \leftarrow \min(\mu, \inf \{ \mu > 0 : \|d(v, \mu)\|_{\infty} \le 1 \})$ \\
    $\alpha \leftarrow \max(1, \frac{1}{2\beta}{\|d(v, \mu)\|^2_{\infty}})$ \\
    $v \leftarrow v + \frac{1}{\alpha} d(v, \mu)$\\
  }
    \Return{$(v, \mu)$}\\
}
\end{algorithm}
  \end{minipage}
  \caption{Algorithms for finding an approximate solution $x(v, \mu)$ to the QP~\eqref{cp:coneRelax}.
  For $(k, N)$ and $(v_0, \mu_0)$ specified by Theorem~\ref{thm:barrier}, the algorithm~{\tt shortstep}
  stays within the quadratic-convergence region of Newton's 
  method~(\Cref{thm:newtonConv}) and terminates
  in $\bigO( \sqrt m \log (\mu_0 \mu^{-1}_f) )$ Newton steps.
  For any step-size parameter $\beta \in [\frac{1}{2}, 1)$,  % chktex 9
 the algorithm {\tt longstep} % checktex ##
  terminates given arbitrary initialization points (\Cref{thm:longstep}),
 and allows for construction of a $\mu_f$-sub-optimal feasible-point $x(v, \mu)$.
 }\label{fig:algorithms}
\end{figure}
The analysis of  Newton's method (\Cref{sec:Newton}) yields two
concrete IPMs (Figure~\ref{fig:algorithms}) for solving the QP~\eqref{cp:coneRelax}. 
The first is a \emph{short-step} algorithm:
it conservatively updates $\mu$, takes full Newton steps, and never
leaves the quadratic-convergence region of Newton's method.
The next is a \emph{long-step} algorithm: it aggressively updates $\mu$
via line-search and takes potentially damped steps.
The first algorithm, {\tt shortstep}, has an $\bigO(\sqrt m)$ iteration
bound, which is typical for interior-point methods.  The second algorithm, {\tt
longstep}, is intended for practical implementation.  


\subsection{Short-step algorithm}
The algorithm {\tt shortstep} reduces the centering parameter $\mu$ by a
fixed-factor $k$ after every $N$ Newton steps. With proper selection of $k$
and $N$, it updates a given centered-point $\hat v(\mu_0)$  to $\hat v(\mu_f)$ 
using at most $C  \sqrt m \log (\mu_0 {\mu_f}^{-1})$
iterations, where $C$ is an explicit constant.  If the quadratic objective term
is zero ($W = 0$), it reduces to the short-step algorithm
of~\cite[Section 2]{permenter2020geodesic}. Its analysis is also identical once
we show that a key divergence bound still holds for non-zero $W$.

To begin, let $q(t) := 2 (\cosh(t) - 1)$. The next lemma establishes the
aforementioned bound and reduces to~\cite[Theorem 3.1]{permenter2020geodesic}
when $W = 0$.
\begin{lem}\label{lem:cpdist}
  For all $\mu_1, \mu_2 > 0$, the centered points $\hat v(\mu_1)$ and $\hat v(\mu_2)$
  satisfy
\[
  \frac{1}{m} h(\hat v(\mu_1), \hat v(\mu_2)) \le q(\frac{1}{2} \log  \frac{\mu_1}{\mu_2})
\]
\end{lem}
\begin{proof}
  Let $w_1 = e^{\hat v(\mu_1)}$ and $w_2 = e^{\hat v(\mu_2)}$
  and let $k_1 = \sqrt{\mu_1}$ and $k_2 = \sqrt{\mu_2}$.
  By primal-dual feasibility,
\[
   b = k_1 w^{-1}_{1} - Ax_1 = k_2 w^{-1}_{2} - Ax_2.
\]
Taking inner-products with $w_1$ and $w_2$ gives:
\[
  \frac{ k_1 m + w^T_1 A(x_2-x_1)}{k_2} =  w^T_1 w^{-1}_{2} , \qquad \frac{ k_2 m + w^T_2 A(x_1-x_2)}{k_1} =  w^T_2 w^{-1}_{1} 
\]
Adding  and simplifying yields:
  \begin{align*}
    h(\hat v(\mu_1), \hat v(\mu_2)) + 2m  &:= w^T_1 w^{-1}_{2}  + w^T_2 w^{-1}_{1}  \\
                                          &=  \frac{(k^2_2 + k^2_1) m + (k_1w^T_1  - k_2w^T_2 ) A(x_2-x_1)}{k_1 k_2} \\
                                          &= \frac{(k^2_2 + k^2_1)  m + (c + Wx_1   -(c+ W x_2) )^T(x_2-x_1)}{k_1 k_2} \\
                                          &= \frac{(k^2_2 + k^2_1)  m + (W(x_1-x_2) )^T(x_2-x_1)}{k_1 k_2} \\
                                          &= \frac{(k^2_2 + k^2_1)  m - (x_1-x_2)^{T}W(x_2-x_1)}{k_1 k_2} \\
                                          &\le \frac{(k^2_2 + k^2_1)  m }{k_1 k_2} 
                                          = m(\frac{k_2}{k_1} +  \frac{k_1}{k_2})
                                          = 2m \cosh(\log \frac{k_1}{k_2}) 
                                          = 2m \cosh(\log \sqrt{\frac{\mu_1}{\mu_2}}) 
  \end{align*}
Subtracting $2m$ proves the claim.
\end{proof}
\noindent We also need a lemma from~\cite{permenter2020geodesic} that relates
divergence to Euclidean distance.
\begin{lem}[Lemma 3.2 of~\cite{permenter2020geodesic}]\label{lem:divanddist}
  For all $v_1, v_2 \in \mathbb{R}^m$ it holds that $\|v_1 - v_2\|^2 \le  h(v_1, v_2) \le q(\|v_1-v_2\|)$.
\end{lem}
\noindent Using these lemmas, the $\mu$-update factor $k$ and the inner-iteration count $N$
are selected such that the divergence $h_{\mu}(v)$ remains
bounded at each iteration by a specified $\beta \in (0, \frac{1}{2}]$. % chktex 9
This in turn implies that each Newton step is
quadratically convergent (\Cref{thm:newtonConv}).  
To ensure that $h_{\frac{1}{k} \mu}(v) \le \beta$  just before $\mu$ updates,
 we use the following upper-bound
\begin{align}\label{eq:tri}
  h_{\frac{1}{k} \mu}(v)   \le q( \|v-\hat v(\mu) \| +  \|  \hat v(\mu) -\hat v(\frac{1}{k}\mu) \|),
\end{align}
which follows from~\Cref{lem:divanddist} and the triangle inequality.
Using \Cref{thm:newtonConv} and a specified $\epsilon$,
 the parameter $N$ is chosen to ensure that $\|v-\hat v(\mu) \| \le \epsilon$.
Using Lemma~\ref{lem:cpdist}, the parameter $k$ is chosen to ensure that
$\|  \hat v(\mu) -\hat v(\frac{1}{k}\mu) \| \le q^{-1}(\beta) - \epsilon$,
where $q^{-1} : \mathbb{R} \rightarrow \mathbb{R}_{+}$ denotes
the nonnegative inverse of $q$.
Together with~\eqref{eq:tri}, this implies that  $h_{\frac{1}{k} \mu}(v) \le \beta$. 

A formal statement of the $(k, N)$-selection criteria and the convergence
guarantees of {\tt shortstep} follow. We omit proof, as it is identical
to~\cite[Theorem~2.1]{permenter2020geodesic}.
\begin{thm}~\label{thm:barrier}
  Let  {\tt shortstep} (\Cref{fig:algorithms}) have parameters $(k, N)$ that satisfy, for
  some  $\frac{1}{2} \ge  \beta > 0$ and $\qup^{-1}(\beta) > \epsilon > 0$, the conditions
  \begin{align}\label{eq:mainass}
    \beta^{2^N}  \le \epsilon^2, \qquad \frac{1}{2} \log k = \qup^{-1}(\frac{1}{m} \zeta^2),
  \end{align}
  where $\zeta := \qup^{-1}(\beta) - \epsilon$.   
  The following statements hold for {\tt shortstep} given input $(\hat v(\mu_0), \mu_0, \mu_f)$: 
  \begin{enumerate}[label= (\alph*)]
      \item At most  $N \lceil c^{-1} \sqrt{m} \log \frac{\mu_0}{\mu_f} \rceil$ Newton steps execute,
        where $c := 2\qup^{-1}(\zeta^2)$.
      \item  The output $(v, \mu)$ satisfies $\|v-\hat v(\mu)\| \le \epsilon$ and $\mu \le \mu_f$.
    \end{enumerate}
\end{thm}
\noindent 
Observe that {\tt shortstep} takes only full Newton-steps  and that
its convergence guarantees assume a centered initialization point, i.e., that $v_0 = \hat v(\mu_0)$. 
The next algorithm {\tt longstep} will support arbitrary initialization through the use
of damped Newton steps.

\subsection{Long-step algorithm}~\label{sec:longstep}
The procedure {\tt longstep}~(\Cref{fig:algorithms})  supports
arbitrary initialization and performs more aggressive updates of 
the centering parameter $\mu$.
At each
iteration, it finds the smallest $\mu$ for
which the Newton direction $d(v, \mu)$ satisfies $\|d(v, \mu)\|_{\infty} \le 1$, if
such a $\mu$ exists.  It terminates once both $\|d(v, \mu)\|_{\infty} \le 1$ and $\mu \le \mu_f$.
The condition $\|d(v, \mu)\|_{\infty} \le 1$ is motivated by the following lemma,
which shows that under this condition, the approximate solution $x(v, \mu)$
associated with the Newton direction (Definition~\ref{defn:NewtonDirection})
is feasible and has  bounded sub-optimality.
\begin{lem}\label{lem:feas}
  For $\mu > 0$ and $v \in \mathbb{R}^m$, let $d = \newton(v, \mu)$ and $x = x(v, \mu)$.  
  Let  $\lambda = \sqrt\mu(e^v + e^v \circ  d)$ and $s = \sqrt\mu(e^{-v} - e^{-v} \circ  d)$.
  If $\|d\|_{\infty} \le 1$,  then $(x, s, \lambda)$ satisfies the primal-dual feasibility conditions
\begin{align*}
Ax + b = s,  \;\;A^T \lambda = Wx + c, \;\; \lambda \ge 0, \;\;s \ge 0.
\end{align*}
  Further,  $\|s \circ \lambda\|_1   = \mu (m - \|d\|^2)$.
  \begin{proof}
  The equality constraints hold by definition of $\newton(v, \mu)$.
  Nonnegativity of both $s$ and $\lambda$ hold by their definition and the
    fact that $\|d\|_{\infty} \le 1$.
  Finally, since $s, \lambda \ge 0$, we have that $\|\lambda \circ s\|_1 = \langle s, \lambda \rangle$.
    Expanding $\langle s, \lambda \rangle$ using the definition of $s$ and $\lambda$
    proves the claim.
   %Further,  $\|s \circ \lambda\|_1   = \mu (m - \|d\|^2)$.
   % \[
   %   \ones + d \ge  \ones - \ones \|d\|_{\infty} \ge \ones(1-\|d\|_{\infty}) 
   % \]

   % \[
   %   \ones - d \ge  \ones - \ones \|d\|_{\infty} \ge \ones(1-\|d\|_{\infty}) 
   % \]
   % Hence, if  $\|d\|_{\infty} \le 1$, then $\ones - d \ge 0$ and $\ones + d \ge 0$.
   %

   % Suppose feasibility holds.  Then,
   % \[
   % \ones + d \ge 0,  \ones - d \ge 0.
   % \]
   % Rearranging shows that $1 \ge d_i \ge -1$, implying that $\|d\|_{\infty} \le 1$.
  \end{proof}
\end{lem}
\noindent It remains to show that the algorithm will actually terminate.
To prove this, we note that the algorithm,
by design, monotonically decreases $\mu$ and always 
applies an update $v \leftarrow v + \frac{1}{\alpha}d(v, \mu)$
with $\|d(v, \mu)\|_{\infty} \ge 1$. We will show 
that infinitely many iterations, and the resulting 
convergence of $\mu$, contradicts $\|d(v, \mu)\|_{\infty} \ge 1$.
Our analysis also selects the step-size parameter $\beta$ 
to ensure global convergence (Theorem~\ref{thm:globalconv}) and full Newton steps  
when $\|d(v, \mu)\|_{\infty} \le 1$.
\begin{thm}~\label{thm:longstep}
  For any step-size parameter $\beta \in [\frac{1}{2}, 1)$ % chktex 9 
  and input $(v_0, \mu_0, \mu_f) \in \mathbb{R}^m \times \mathbb{R} \times \mathbb{R}$ with $\mu_0 > \mu_f > 0$,
  the algorithm {\tt longstep} terminates and returns $(v, \mu)$ with $\mu \le \mu_f$.
  Further, letting $x$ denote $x(v, \mu)$, we have that
  \[
    Ax+ b \ge 0,    \qquad \frac{1}{2} x^T  W x + c^{T}x \le V^* +  \mu  m,
  \]
  where $V^*$ denotes the optimal value of QP~\eqref{cp:coneRelax}.
  \begin{proof}

    %Each step $i$ of the algorithm performs the update $v_{i+1} \leftarrow v_i + \frac{1}{\alpha_i} d_i$
    %with $\|d_i\|_{\infty} \ge 1$.  Further, $\mu_i$ is bounded below and monotonically decreasing; hence, 
    %if the algorithm does not terminate, $\mu_i$ converges to some $\bar\mu$.  
    %This implies that $v_i$ converges to $\hat v(\bar \mu)$.
    %Hence, at some iteration $N$, we have that  $1 > \|v_{N+1}-v_{N}\|_{\infty} = \|d(v_N, \mu_N)\|_{\infty}$, 
    %a contradiction.

    Let $v_i$, $\mu_i$ and $\alpha_i$ denote the sequences generated
    by {\tt longstep} indexed by $i$ such that
    $v_{i+1} = v_i + \frac{1}{\alpha_i}d(v_i, \mu_{i})$.
    Now suppose the algorithm does not terminate.
    We first consider the case where $\mu$ updates only finitely many times.
    In this case, there exists an $M$ such that 
      $\mu_i = \mu_{M}$ for all iterations $i > M$,
      which implies that $v_i$ converges to $\hat v(\mu_M)$ by Theorem~\ref{thm:globalconv}.
      But by selection of $\mu_i$, we also have  that $\|d(v_i, \mu_i)\|_{\infty} \ge 1$ for all $i$,
    a contradiction. Hence, in this case, the algorithm must terminate.

    Now suppose that $\mu$ updates infinitely many times
    and  let $\sigma_k$ denote the subsequence of iterations $i$
    where $\mu$ changes, i.e., $\mu_{\sigma_{k}} <  \mu_{\sigma_{k-1}}$
    and $\mu_i = \mu_{\sigma_{k-1}}$ for $  \sigma_{k} > i \ge  \sigma_{k-1}$.
    For brevity, let $h_{k} : \mathbb{R}^m \rightarrow \mathbb{R}$ denote the 
    divergence $h(v,  \hat v(\mu_{\sigma_k}))$ as a function of $v$.
    We note that
    \begin{align}~\label{eq:convProof}
      \frac{1}{2} h_{k}(v_{\sigma_k}) \ge h_{k}(v_{1 + \sigma_k})   \ge        h_{k}(v_{\sigma_{k+1}}),
    \end{align}
    where the first inequality holds by Theorem~\ref{thm:newtonstep} since
    $\|d(v_i, \mu_i)\|_{\infty} = 1$ for $i = \sigma_k$ and the second because
    Newton iterations decrease $h_{\mu}(v_i)$ for fixed $\mu$.

    Since $\mu_{\sigma_k}$ is bounded below and monotonically decreasing, it converges. 
    Since the map $\mu \mapsto \hat v(\mu)$ is continuous,
    the sequence $\hat v(\mu_{\sigma_{k+1}})$ also converges.
    Hence, for any $\epsilon > 0$, there exists an $N$ such that 
    for all $k > N$,
    \[
      \|\hat v(\mu_{\sigma_k}) - \hat v(\mu_{\sigma_{k+1}})\| < \epsilon.
    \]
   This shows that for $k > N$,
    \begin{align*}
      h_{k+1}(v_{\sigma_{k+1}}) + 2m 
                                            &=  2 \sum^m_{j=1} \cosh( [v_{\sigma_{k+1}} - \hat v(\mu_{\sigma_{k+1}})]_j  ) \\
                                            &\le  2 \sum^m_{j=1} \cosh( |[v_{\sigma_{k+1}} - \hat v(\mu_{\sigma_{k}})]_j|+ \epsilon ) \\
                                            &\le 2 \sum^m_{j=1} \cosh( [v_{\sigma_{k+1}} - \hat v(\mu_{\sigma_{k}})]_j ) (\cosh(\epsilon) + \sinh(\epsilon)) \\
                                            & =   (h_{k}(v_{\sigma_{k+1}}) + 2m)  (\cosh(\epsilon) + \sinh(\epsilon)) \\
                                            & \le   (\frac{1}{2} h_{k}(v_{\sigma_{k}}) + 2m)  (\cosh(\epsilon) + \sinh(\epsilon)),
    \end{align*}
    where the third line uses the inequality $\cosh(x+y) \le \cosh(x)( \cosh(y) + \sinh(|y|))$
    and the last uses~\eqref{eq:convProof}. 
    Letting $C(\epsilon) = \cosh(\epsilon) + \sinh(\epsilon)$, we rearrange the last line to conclude that 
    \[
      h_{k+1}(v_{\sigma_{k+1}}) \le  \frac{C(\epsilon)}{2} h_{k}(v_{\sigma_{k}}) + 2m(C(\epsilon)-1)
    \]
    This shows that $h_{k}(v_{\sigma_{k}})$ is upper-bounded by 
    a sequence of the form $a_{k+1} = c_1(\epsilon) a_{k} + c_2(\epsilon)$,
    which, if $|c_1| < 1$,  converges to $L = \frac{c_2}{1-c_1}$.
    For any $\delta > 0$, we can pick $\epsilon$ small enough to show that $L < \delta$.
    This shows that $h_{k}(v_{\sigma_{k}})$ converges to zero, contradicting $\|d(v_{\sigma_{k}}, \mu_{\sigma_{k}})\| \ge 1$
    by \Cref{lem:divBound}.  Hence, the algorithm must terminate.
    Finally, the feasibility and suboptimality guarantees for $x(v, \mu)$
    follow from \Cref{lem:feas} and weak duality.
  \end{proof}
\end{thm}
\noindent We conclude with three topics related to selection of $\mu$.
\subsubsection{Computing the infimum}
Fix $v \in \mathbb{R}^m$ and let $d(\mu)$ denote $d(v, \mu)$.  
In this notation,
each iteration of {\tt longstep} requires computation of
$\inf \{ \mu > 0 : \|d(\mu)\|_{\infty} \le 1 \}$.
This is straight-forward upon recognition that $d(\mu)$ is an affine function
of $(\sqrt\mu)^{-1}$, i.e., it decomposes as $d(\mu) = d_0 + (\sqrt\mu)^{-1} d_1$
for some fixed $d_0\in \mathbb{R}^m$ and $d_1\in \mathbb{R}^m$.
We give a constructive proof of this fact that demonstrates how to
build this decomposition.
\begin{prop}\label{prop:ddecomp}
  There exists $d_0, d_1 \in \mathbb{R}^m$ satisfying
    $\newton(\mu) =  d_0  + \frac{1}{\sqrt \mu} d_1$ for all $\mu > 0$.
 \begin{proof}
 Fix $\mu_1 > 0$ and $\mu_2 > 0$ and $v \in \mathbb{R}^m$.
 Let $\hat d_i = \newton(v, \mu_i)$ and $k_i = \frac{1}{\sqrt\mu_i}$ for $i \in {1, 2}$.
   By Definition~\ref{defn:NewtonDirection}, there exists $\hat x_1$ and $\hat x_2$ satisfying
   \[
     A^T (e^v+ e^v \circ \hat d_1) = W \hat x_1 + k_1 c, \qquad A^T (e^v+ e^v \circ \hat d_2) = W \hat x_2 + k_2 c.
   \]
   Multiplying these equations by $t$ and $(1-t)$, respectively,
   and adding yields
   \[
     A^T (e^v+ e^v \circ (t \hat d_1 + (1-t) \hat d_2 )) = W (t \hat x_1  + (1-t)\hat x_2 ) + (t k_1 + (1-t) k_2) c.
   \]
   By similar argument,
   \[
      e^{-v} -  e^{-v} \circ  (t \hat d_1 + (1-t) \hat d_2 )   = A(t \hat x_1  + (1-t)\hat x_2 ) +  (t k_1 + (1-t) k_2)  b.
  \]
    By Definition~\ref{defn:NewtonDirection}, we conclude that $t \hat d_1 + (1-t) \hat d_2 = d(v, \mu)$ for
    $\frac{1}{\sqrt\mu} = t k_1 + (1-t) k_2$. Solving for $t$ shows that
     $t  = c_1 \frac{1}{\sqrt\mu} + c_0$ for $c_1 = (k_1-k_2)^{-1}$ and $c_0 = -k_2(k_1-k_2)^{-1}$.
   Substituting into $t \hat d_1 + (1-t) \hat d_2$, we deduce that
   \[
     d(v, \mu) = (c_1 \frac{1}{\sqrt\mu} + c_0) \hat d_1 + (1-c_1 \frac{1}{\sqrt\mu} - c_0) \hat d_2.
   \]
   Hence, the claim follows for $d_0 = c_0 \hat d_1 + (1-c_0) \hat d_2$
   and $d_1 = c_1 (\hat d_1 - \hat d_2)$.

%  Then (\frac{1}{\sqrt \mu_1} - \frac{1}{\sqrt \mu}) (\frac{1}{\sqrt \mu_1} - \frac{1}{\sqrt \mu_2})^{-1}
%
%  Then for all $\mu > 0$, the Newton direction $\newton(v, \mu)$ satisfies
%  \[
%    \newton(v, \mu) =  d_1  + t(\mu) (d_2 - d_1) 
%  \]
%  where $t(\mu) := $.
  \end{proof}
\end{prop}
This decomposition in turn allows us to  characterize the condition
$\|d(\mu)\|_{\infty} \le 1$ using a system of linear inequalities
immediate from the definition of $\|\cdot\|_{\infty}$.
  \begin{prop}~\label{prop:mustar}
    For $d_0, d_1 \in \mathbb{R}^m$, 
    we have that $\|d_0 + \frac{1}{\sqrt\mu} d_1\|_{\infty} \le 1$ if and only if
    %\max_{i \in S}  \min(\frac{1 - d_{0,i}}{d_{1,i}}, \frac{-1 - d_{0,i}}{d_{1,i}}) \le
    %\frac{1}{\sqrt\mu}  \le \min_{i \in S}  \max(\frac{1 - d_{0,i}}{d_{1,i}}, \frac{-1 - d_{0,i}}{d_{1,i}})
    \begin{align}\label{eq:mustar}
      -\ones \le d_0 +  \frac{1}{\sqrt\mu} d_1 \le \ones ,
  \end{align}
    where $\ones \in \mathbb{R}^m$ denotes the vector of all ones.
  \end{prop}
\noindent Note that minimizing $\mu$ subject to these inequalities can be done
in $\bigO(m)$ time simply by iterating over the components of $d_0, d_1 \in \mathbb{R}^m$.
\subsubsection{Reuse of factorizations}
The constructive proof of \Cref{prop:ddecomp}  
builds the decomposition $d_0 + (\sqrt\mu)^{-1} d_1$
from two Newton directions $d(v, \mu_1)$ and $d(v, \mu_2)$.  Since $v$ is \emph{fixed}, 
these directions are found by solving 
two Newton systems (\Cref{defn:NewtonDirection}) with the \emph{same} 
positive definite coefficient matrix $W + A^T Q(v) A$.  
Hence, one can find both directions using the same Cholesky factorization
of $W + A^T Q(v) A$.
%  This is efficiently done by factorizing $S = L L^T$ for a lower-triangular
%matrix $L$ and applying the inverses of $L$ and $L^T$ via forward and backward substitution.
\subsubsection{Least-squares $\mu$}\label{sec:least_squares}
If $d^T_0 d_1 < 0$, then the decomposition $d(\mu) = d_0 + (\sqrt\mu)^{-1} d_1$
of \Cref{prop:ddecomp}
also enables easy computation of the least-squares $\mu$, i.e., the $\mu$ 
that minimizes $\|d(\mu)\|^2$.  This $\mu$ is the 
solution of  $-d^T_0 d_1  \sqrt\mu =\|d_1\|^2$
   and provides a natural heuristic choice for the initial $\mu_0$ passed to {\tt longstep}.
%For this $\mu$, one can also show that $\|d(\mu)\|^2 \le m$.
%\begin{prop}
%  Let $\mu >0 $ satisfy~\eqref{eq:lesmu}.  Then $\|d(\mu)\|^2 \le m$.
%  \begin{proof}
%    By our choice of $\mu$, we have that $\|d(\mu)\| \le \|d_0\|$ since $\inf_{k} \|d_0 + k d_1 \| \le \|d_0\|$.
%    We will show that $\|d_0\|^2 < m$. To begin,
%\[
%  \langle e^{-v} -  e^{-v} \circ d_0, e^{v} +  e^{v} \circ d_0\rangle = \|\ones\|^2 - \|d\|^2
%\]
%    Further, by \Cref{thm:newtonstep},
%  \[
%    d(v, \mu) = \ones -   e^v \circ (Ax +  \frac{1}{\sqrt\mu}  b),  \qquad (A^T Q(v) A +  W)x = 2 A^{T} e^v  -  \frac{1}{\sqrt \mu}  (c+ A^{T}Q(v)b).
%  \]
%Setting $\frac{1}{\sqrt\mu} = 0$, we deduce that
%    \[
%      d_0 = \ones - e^v \circ (Ax), \qquad Wx = 2 A^{T}( e^v   + e^v)
%    \]
%We also have for some $x \in \mathbb{R}^m$ that 
%\[
%    A^T(e^v +  e^v \circ d_0) =  Wx,\;\; \qquad           \\
%    (e^{-v} -  e^{-v} \circ d_0)  = Ax 
%\]
%Combining these facts gives
%\[
%  \|\ones\|^2 - \|d\|^2 = \langle   Ax, e^{v} +  e^{v} \circ d_0\rangle  = \langle   x, A^T(e^{v} +  e^{v} \circ d_0) \rangle = x^{T}Wx,
%\]
%which proves the claim.
%  \end{proof}
%\end{prop}
%Note that in this formula we have ignored the implicit constraint that $k =
%\frac{1}{\sqrt\mu} \ge 0$.  Unfortunately, we cannot provide an priori guarantee 
%that $k$ is nonnegative.

\section{Computation}~\label{sec:comp}
The  QP~\eqref{cp:coneRelax} can be reformulated as the following second-order cone
program  (SOCP)
\begin{align}\label{eq:socp}
  \begin{aligned}
  \mbox{minimize  } t  + c^T x   \\
  Ax + b &\ge 0 \\
  (t+1/2, t-1/2, W^{1/2} x) &\in \mathbb{L}^{n+2},
  \end{aligned}
\end{align}
where $t$ is an additional decision variable and $\mathbb{L}^{n+2}$ denotes $\{ ( x_0, y_0, z) \in \mathbb{R} \times \mathbb{R} \times \mathbb{R}^n : x_0 \ge 0, x^2_0 \ge y^2_0 + \|z\|_2^2\}$.
This widely used reformulation allows one to solve QPs using \emph{cone} solvers
such as SeDuMi~\cite{sturm1999using} and~SDPT3~\cite{toh2009sdpt3} and is automated by parsers such as YALMIP~\cite{YALMIP}
and CVX~\cite{grant2008cvx}.  The \emph{geodesic interior-point
method}~\cite{permenter2020geodesic} solves SOCPs by generalizing the
log-transformation of the central-path conditions
(Section~\ref{sec:ipm_template}). Hence, it is natural to compare its
performance on~\eqref{eq:socp} with the techniques developed here. This section
performs this comparison and illustrates that solving QP~\eqref{cp:coneRelax} with
{\tt longstep} (\Cref{sec:alg}) requires
fewer iterations than the geodesic IPM applied to~\eqref{eq:socp}. 


\subsection{Instances}
Our comparison uses randomly generated QPs that satisfy the regularity
conditions of Assumption~\ref{ass:main}.  The inequality matrix $A$ 
has entries drawn from a normal distribution with zero mean
and unit variance. Each row is then rescaled to have unit norm.
The cost matrix $W$ is constructed as $W = R^T R$ with $R \in \mathbb{R}^{r \times n}$
sampled and normalized the same way as $A$. Finally $c$ and $b$ are chosen
by sampling $x$, $\lambda > 0$ and $s > 0$  and setting $b = s - Ax$,  $c = A^{T}\lambda  - Wx$.
The vector $x$ is drawn from a normal distribution and rescaled to have
unit norm.  The vector $s$  is chosen as $\ones + \frac{1}{10} (\sqrt m)^{-1} w$, where $w$ is 
also sampled from a normal distribution. Finally, $\lambda$ is chosen the same way as $s$
using a different random $w$.

\subsection{Results}
\Cref{tab:results} shows superior performance of {\tt longstep} on
a set of instances. In this set, we fix the number of variables $n$ to 100 
and vary either the rank of $W \in \mathbb{R}^{n \times n}$ or the number of inequalities $m$, i.e.,
the number of rows of $A \in \mathbb{R}^{m \times n}$. The rank of $W$ is controlled by the
number of columns $r$ of $R \in \mathbb{R}^{r \times n}$, recalling that  $W = R^T R$.
 The algorithm {\tt longstep} is initialized with $v_0 = 0$,   $\mu_f = 10^{-4}$,
 and $\mu_0 = \mu_{ls}$, where $\mu_{ls}$ denotes the least-squares $\mu$ (\Cref{sec:least_squares}).
 The geodesic IPM is initialized analogously.

%\subsection{Software}
%Code for reproducing experiments and running both algorithms is available
%at 
%\[
%\mbox{\tt www.github.com/frankpermenter/conex/examples/cpp/quadratic\_programs}.
%\]
%An implementation of {\tt longstep} is also the default algorithm for solving
%QPs with {\tt conex}, an open-source optimization package. 

% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 25 Avg Iter QP:  5.5 Avg Iter SOCP:   6.5
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 50 Avg Iter QP:  8.75 Avg Iter SOCP:  16.18
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 75 Avg Iter QP:  8.39 Avg Iter SOCP:  19.27
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 100 Avg Iter QP: 7.96 Avg Iter SOCP: 21.01
%


% Num instances: 100 Num Vars: 100 Num Ineq: 25 Rank: 75 Avg Iter QP:  5.43 Avg Iter SOCP: 6.52
% Num instances: 100 Num Vars: 100 Num Ineq: 50 Rank: 75 Avg Iter QP:  7.29 Avg Iter SOCP: 15.26
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 75 Avg Iter QP:  8.39 Avg Iter SOCP: 19.27
% Num instances: 100 Num Vars: 100 Num Ineq: 100 Rank: 75 Avg Iter QP: 8.53 Avg Iter SOCP: 21.17

%Conex commit: bf60bd6bdcfdd8ae7d08b1506c138363decd5073
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 25 Avg Iter QP:  6.48 Avg Iter SOCP:  6.53
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 50 Avg Iter QP:  8.72 Avg Iter SOCP:  16.11
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 75 Avg Iter QP:  8.38 Avg Iter SOCP:  19.26
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 100 Avg Iter QP: 8.05 Avg Iter SOCP: 20.96
% Num instances: 100 Num Vars: 100 Num Ineq: 25 Rank: 75 Avg Iter QP:  6.54 Avg Iter SOCP:   6.45
% Num instances: 100 Num Vars: 100 Num Ineq: 50 Rank: 75 Avg Iter QP:  7.24 Avg Iter SOCP:  15.16
% Num instances: 100 Num Vars: 100 Num Ineq: 75 Rank: 75 Avg Iter QP:  8.38 Avg Iter SOCP:  19.26
% Num instances: 100 Num Vars: 100 Num Ineq: 100 Rank: 75 Avg Iter QP: 8.47 Avg Iter SOCP: 21.17



\begin{table}
  \centering
  \smaller{}
  \begin{tabular}{rrrr}
    \# ineq.   &  $\rank W$ & iters/LS& iters/GIPM  \\
    \toprule
   75&   25 &  6.48 &  6.53 \\ 
   75 &  50 &  8.72 &  16.11 \\
   75 &  75 &  8.38 &  19.26 \\
   75 &  100 & 8.05 &  20.96 \\
  \end{tabular}
  \qquad
  \begin{tabular}{rrrr}
    \# ineq.   &  $\rank W$ & iters/LS& iters/GIPM  \\
    \toprule
  25   &  75 & 6.54 &  6.45 \\ 
  50   &  75 & 7.24 & 15.16 \\
  75   &  75 & 8.38 & 19.26 \\
  100  &  75 & 8.47 & 21.17 \\
  \end{tabular}
  \caption{Comparison of {\tt longstep} (LS)  on the QP~\eqref{cp:coneRelax} with the geodesic IPM (GIPM)
  on the equivalent SOCP~\eqref{eq:socp}. We report average iterations
  over 100 instances. }\label{tab:results}
\end{table}


%\[
%  t \ge \frac{1}{2} x^T W x \Leftrightarrow  (t+1/2)^2 \ge (t-1/2)^2 + x^T W x
%\]
%Concretely, we obtainx
%\begin{align*}
%  \minimize_{t, x} t  + c^T x   \\
%  Ax + b &\ge 0 \\
%  (t+1/2, t-1/2, W^{1/2} x) &\in \mathbb{L}^{m+1}.
%\end{align*}
%We compare performance of the log-domain IPM with 
%itj SOCP analogue---the \emph{geodesic IPM}. 


%\section{Extension to symmetric cones}~\label{sec:sym}
{\small
\bibliographystyle{abbrvnat}
\bibliography{bib}
}

\end{document}
Near optimum:
  \[ 
  (A^T Q(v) A +  W)x = 2\sqrt \mu  A^{T} e^v  -(c+ A^{T}Q(v)b),
  \]

Substituting $x = x_* + \Delta x$ and using
\[
  W x_* = -c, \qquad A^T Q A x_* + A^{T}Q b = A^T Q s_*
\]
  \[ 
  (A^T Q(v) A +  W)\Delta x = 2\sqrt \mu  A^{T} e^v  -A^{T}Q(v)s_*,
  \]

If $ 2 \sqrt\mu e^{-v} = s_*$, then
\[
  2\sqrt \mu  A^{T} e^v  - A^{T}Q(v)s_*  = \Delta x = 0.
\]
\[
  d = e - \frac{1}{\sqrt\mu} s_* \circ e^v = -2e
\]

\begin{lem}
  If $\sqrt\mu e^{-v} = s_*$, then $\|d\| = 1$
  and $x(v, \mu) = x_*$.
\end{lem}


\begin{lem}
  For an optimal solution $x_*\in\mathbb{R}^m$, let $s_* = A x_*  + b$
  and assume that $s_* > 0$.
  For $\eta > 0 $ and $\gamma \in \mathbb{R}^m$ let $s = \sqrt\eta e^{-\gamma}$
  and define $r = s_* \circ s^{-1}$.
Then
  \begin{align}\label{eq:newtonRatioFormula}
    d(\gamma, \eta) = \ones - r - 2 e^{\gamma} \circ A [W+A^{T}Q(\gamma)A]^{-1} A^T e^{\gamma} (\ones - \frac{1}{2}r).
  \end{align}
  \begin{proof}
     By definition, the Newton direction $d$ and associated $x$ satisfy
    \[
      d = \ones - \frac{1}{\sqrt\eta}  e^{\gamma} \circ(A x + b),   \qquad (A^T Q(\gamma) A +  W) x = 2 \sqrt\eta A^T e^\gamma - (c + A^T Q(\gamma) b).
    \]
    Letting $\Delta x = x - x_*$, we can rewrite $d$ in terms of $\Delta x$ as
    \begin{align}
      d &= \ones - \frac{1}{\sqrt\eta} e^{\gamma} \circ ( A x_* + A \Delta x + b) \nonumber\\
        &= \ones - r - \frac{1}{\sqrt\eta} e^{\gamma} \circ (  A \Delta x ) \label{eq:proofNewtonRatio}. 
    \end{align}
  Observing that
\[
  W x_* = -c, \qquad Q(\gamma)s_* = e^\gamma \circ (e^\gamma \circ s_*) = \sqrt\eta e^\gamma \circ r,
\]
we also conclude
    \begin{align*}
      (A^T Q(\gamma) A +  W)\Delta x  &= 2 \sqrt\eta A^T e^\gamma - (c + A^T Q(\gamma) b) - (A^T Q(\gamma) A +  W)x_*\\
        &= 2\sqrt \eta  A^{T} e^\gamma  -A^{T}Q(\gamma)s_* \\
                             &=  2\sqrt\eta  A^{T} e^\gamma(\ones - \frac{1}{2}r),
    \end{align*}
    Solving for $\Delta x$ and substituting into~\eqref{eq:proofNewtonRatio} proves the claim.
    %\[
    %  d = e - r - 2 e^{\gamma} \circ   A (A^T Q(\gamma) A +  W)^{-1} A^{T} (e^\gamma\circ( \ones - \frac{1}{2}r))
    %\]
    %Letting $\gamma = -\log \hat s - \alpha$


    %This shows we want to make $\gamma$ as negati\gammae as possible.
    %We can do this without changing $r$ by decreasing $\eta$ so that
    %\[
    %  r = \frac{1}{\sqrt\eta_1} e^\gamma = \frac{1}{\sqrt\eta_2} e^{\gamma + \alpha e}
    %\]
    %We pick
    %\[
    %  \sqrt \eta_2 = \sqrt\eta_1 e^{\alpha}
    %\]
  \end{proof}
\end{lem}


For a given $s > 0$, there there are infinity many pairs $(\gamma, \eta)$ satisfying
$s = \sqrt\eta e^{-\gamma}$. 
The next corollary shows we can choose a pair that makes $\|d(\gamma, \eta)\|_{\infty}$ 
arbitrary close to $\|e-r\|_{\infty}$, where $r$ denotes the ratio $s^{-1} \circ s_*$.
Hence, if we know an $s$ that is close to $s_*$, 
we can pick $(\gamma, \eta)$ satisfying $\|d(\gamma, \eta)\|_{\infty}  < 1$, ensuring
a good warmstart. 
\begin{cor}
  For an optimal solution $x_*\in\mathbb{R}^m$, let $s_* = A x_*  + b$
  and assume that $s_* > 0$.  For $s \in \mathbb{R}^m$ with $s > 0$, let $r = s_* \circ s^{-1}$.
  Finally, define
  \[
    \gamma(\alpha) :=  -(\log s + \alpha \ones), \qquad \eta(\alpha) = e^{-2\alpha}.
  \]
  Then, $s = \sqrt{\eta(\alpha)} e^{-\gamma(\alpha)}$ for all $\alpha$.
  Further,
  \[
    \lim_{\alpha \rightarrow \infty} \|d(\gamma(\alpha), \eta(\alpha))\|_{\infty} \le \|e-r\|_{\infty}
  \]
\begin{proof}
  That $s = \sqrt{\eta(\alpha)} e^{-\gamma(\alpha)}$  is trivial.
From~\eqref{eq:newtonRatioFormula},
  \begin{align*}
    \|d(\gamma, \eta)\|_{\infty} &\le \|\ones - r\|_{\infty} + \| 2 e^{\gamma} \circ A [W+A^{T}Q(\gamma)A]^{-1} A^T e^{\gamma} (\ones - \frac{1}{2}r)\|_{\infty}\\
                                 &\le \|\ones - r\|_{\infty} + 2 \|e^{\gamma}\|^2  \| A\|^2 \cdot  \|[W+A^{T}Q(\gamma)A]^{-1}\| \|\ones - \frac{1}{2}r)\|\\ 
                                 &\le \|\ones - r\|_{\infty} + 2 k e^{-2\alpha}  \cdot  \|[W+A^{T}Q(\gamma)A]^{-1}\| 
  \end{align*}
  where $k$ is a constant depending on $\log s$, $A$ and $r$.
  Further,
  \[
    \lim_{\alpha \rightarrow \infty} \|[W+A^{T}Q(\gamma)A]^{-1}\| =  \lim_{\alpha \rightarrow \infty} \|W^{-1}\| 
  \]
  Hence, $\lim_{\alpha \rightarrow \infty} \|d(\gamma, \eta)\|_{\infty}  \le \|\ones - r\|_{\infty} +  k_2 e^{-2\alpha}$
  for a constant $k_2$, proving the claim.
\end{proof}
\end{cor}


If $ 2 \sqrt\mu e^{-v} = s_*$, then
\[
  2\sqrt \mu  A^{T} e^v  - A^{T}Q(v)s_*  = \Delta x = 0.
\]
\[
  d = e - \frac{1}{\sqrt\mu} s_* \circ e^v = -2e
\]


\[
  d = \ones - r_{s} + 2 e^v A (A Q(v) A)^{-1} A^T e^v (\frac{1}{2}(r_s + r_{\lambda}) -\ones )
\]

\[
  \sqrt\mu r_s =  s_* \circ  e^v, \qquad  \sqrt\mu r_{\lambda} = \lambda_* \circ  e^{-v}
\]


When the relative errors are all the same,
\[
  r_s + r_{\lambda} = 
\]



Want 
\[
  k_2 \sqrt\mu r_s =  s_* \circ  e^{k_1v}, \qquad  k_2 \sqrt\mu r_{\lambda} = \lambda_* \circ  e^{-k_1 v}
\]

\[
  h( \frac{1}{\mu} v_*,  v)
\]


\[
h_1 = h(  e^{v_l + k e + t(v-e) }  , \lambda ) \qquad h_2 = h(  e^{-v_s + k e - t(v_s+e) }  , s  )
\]


Let $w = e^{v_l + k e}$ and  $d = (v_l-e)$
and $r = \lambda \circ w^{-1}$.
\begin{align*}
  -(f(0) + f'(0) + \|d\|^2) &= \langle  w\circ(e+d)-\lambda,  w^{-1}\circ(e-d)-\lambda^{-1} \rangle\\
                            &= \langle \lambda \circ( r^{-1}(\ones+d) -\ones), \lambda^{-1}\circ(r(\ones-d) - \ones)\rangle\\
                            &= \langle  r^{-1}(\ones+d) -\ones, r(\ones-d) - \ones\rangle\\
                            &= 2 \ones^2 - \|d\|^2  - \langle r^{-1}, 1+d\rangle - \langle r,\ones-d\rangle\\
                            &= -f(0) - \|d\|^2  - \langle r^{-1}, d\rangle - \langle r,-d\rangle
\end{align*}


\[
  \langle r, d \rangle - \langle r^{-1}, d \rangle = 
  \langle r, (v-e) \rangle - \langle r^{-1}, (v-e) \rangle 
\]

\[
                            = \langle  r^{-1}(\ones+d) -\ones, r(\ones-d) - \ones\rangle\\
\]

%
%
%= < lambda( r^{-1}(1+d) -1), lambda^{-1}(r(1-d) - 1)>
% = < r^{-1}(1+d)-1, r(1-d) - 1)
%
%= 2 *|1|^2 - |d|^2  - <r^{-1}, 1+d> - <r,1-d>

We perturbed the problem $x_0$ with $x_0 + e$. 

\[
  w \in k x_0 +   k_p (e - x_0) + \mathcal{L}, \qquad  w^{-1} \in k s_0  + k_p (e - s_0) + \mathcal{L}^{\perp}
\]
From these definition, we deduced that
\[
  2  \langle w, w^{-1} \rangle = k (\langle s_0, w \rangle + \langle x_0, w^{-1} \rangle) +
                                 k_p (\langle e - s_0, w \rangle + \langle e - x_0, w^{-1} \rangle)
                                 - r_p - r_d
\]
where
\[
  r_p = \langle w, k s_0  + k_p (e - s_0) - w^{-1}  \rangle \qquad
  r_d = \langle w^{-1}, k x_0  + k_p (e - x_0) - w  \rangle
\]
To simplify $r_p$, we note that $k s_0  + k_p (e - s_0) - w^{-1}  \in \mathcal{L}^{\perp}$.
Hence,
\[
  r_p = \langle  k x_0 +   k_p (e - x_0)  , k s_0  + k_p (e - s_0) - w^{-1}  \rangle
\]
Assuming $\langle x_0, s_0 \rangle = 0$,
\[
  r_p = \langle  k k_p e, x_0\rangle - k \langle x_0, w^{-1} \rangle
  +  k_p k \langle e, s_0 \rangle
  +  k_p k_p \langle (e - s_0, e - x_0 \rangle
  -  k_p \langle (e - x_0, w^{-1} \rangle  
\]

Letting $g = \langle   x_0, w^{-1} \rangle +  \langle   s_0, w \rangle$
and $\hat g = \langle  e- x_0, w^{-1} \rangle +  \langle  e- s_0, w \rangle$ we
get
\[
  r_p +  r_d = 2k k_p \trace( s_0 + x_0) - k g - k_p \hat g + 2 k^2_p \langle (e - s_0, e - x_0 \rangle
\]
Substituting into
\[
  2  \langle w, w^{-1} \rangle = k g + k_p \hat g - (r_p + r_d)
\]
gives
\[
  2  \langle w, w^{-1} \rangle = 2(k g + k_p \hat g) - ( 2k k_p \trace( s_0 + x_0) + 2 k^2_p \langle (e - s_0, e - x_0 \rangle)
\]


So we pick $k$ and $k_p$ to ensure that $kg \le 0$.
This is achieved if 
\[
k_p \hat g - ( k k_p \trace( s_0 + x_0) + 2 k^2_p \langle (e - s_0, e - x_0 \rangle) \ge 2n
\]





\begin{lem}
  The Newton direction satisfies
  \[
    d = \ones - \frac{1}{\sqrt\mu} e^v \circ (Ax + b) 
  \]
  where $x$ is the minimizer of
  \[
    \frac{1}{2} \| e^{v} \circ(Ax+b)\|^2 + (c - 2 \sqrt\mu A^T e^v)^{T}x
  \]

Suppose  $(c - 2 \sqrt\mu A^T e^v)^{T}x   \ge 0$.
  \[
    \frac{1}{2 \sqrt\mu}    e^v \circ (Ax + b)  \le 1
  \]
  \begin{proof}
  \begin{align*}
    \frac{1}{2} \| e^v \circ (Ax + b)\|^2_{\infty} &\le\frac{1}{2}  \|    e^v \circ (Ax + b)\|^2 +  (c - 2 \sqrt\mu A^T e^v)^{T}x\\ 
                                       &\le            \frac{1}{2}  \|    e^v \circ (Ax_* + b)\|^2 + (c - 2 \sqrt\mu A^T e^v)^{T}x_* \\
                                       &=            \frac{1}{2}  \|    e^v \circ (Ax_* + b)\|^2 +  [A^T(\lambda_* - 2 \sqrt\mu  e^v)^{T}] x_* \\
                                       &=            \frac{1}{2}  \|    e^v \circ (Ax_* + b)\|^2 +  (\lambda_* - 2 \sqrt\mu  e^v)^{T} A x_* \\
  \end{align*}


    So we have 
    \[
      \frac{1}{2 \mu}    \|e^v \circ (Ax + b)\|^2_{\infty} \le 2
    \]
    if 
    \[
      2 \ge \frac{1}{2}  \|   \frac{1}{\sqrt\mu} e^v \circ (Ax_* + b)\|^2 +  \frac{1}{\mu}(\lambda_* - 2 \sqrt\mu  e^v)^{T} A x_*
    \]
    The first term only depends on the inactive set with $A x_* + b > 0$

    This is a dead end since
    \[
      \|   \frac{1}{\sqrt\mu} e^v \circ (Ax_* + b)\|^2 = \Omega(\mbox{\# active constraints})
    \]
    when $(v, \mu)$ is close to optimal.
  \end{proof}

\end{lem}


\begin{lem}
  The Newton direction satisfies
  \[
    d = \ones - \frac{1}{\sqrt\mu} e^v \circ (Ax + b) 
  \]
  where $x$ is the minimizer of
  \[
    \frac{1}{2} \| e^{v} \circ(Ax+b)\|^2 + (\lambda_* - 2 \sqrt\mu  e^v)^{T}(Ax + b)
  \]
\end{lem}







  \begin{itemize}
    \item Give conditions where $\|d(x_*)\|_{\inf} \le 1$ is feasible.
      \[
        Ax_* + b \le 2 \sqrt\mu e^{-v}
      \]
    \item Give condition where $-b < A \delta x < 0$. 
    \item What does $\lambda_* \le 2 \sqrt\mu e^v$ imply? 
      Let $z = e^v \circ (Ax+b)$ and $w = e^{-v} \circ ( \lambda_* - 2 \sqrt\mu  e^v)$.
      Then the function
      \[
        \frac{1}{2} \| e^{v} \circ(Ax+b)\|^2 + ( \lambda_* - 2 \sqrt\mu  e^v)^{T} (Ax + b)
      \]
      can be written as
      \[
        \frac{1}{2} \| z(x) \|^2 + (w)^{T} z(x)
      \]
      where $z(x) = D (Ax + b)$.
      If $\lambda_* \le 2 \sqrt\mu e^v$, then the unconstrained minimizer satisfies $z = -w \ge 0$.
      \[
        A^T D^T (z(x) + w)  = 0
      \]


      Moreover, at optimality,
      \[
        \|\sqrt\mu 2 e^{v} - \lambda_*\| = \Omega(\mu \frac{1}{s}) ,  \quad \sqrt\mu 2 e^{-v} = s_*
      \]


  \end{itemize}

\begin{lem}
  let $(\lambda_*, x_*, s_*)$ be primal-dual feasible.
  For $\mu > 0 $ and $v \in \mathbb{R}^m$ ,
  $r_{s} = s_* \circ (2\sqrt\mu e^{-v})^{-1}$.
  and $r_{\lambda} = \lambda_* \circ (2\sqrt\mu e^{v})^{-1}$.
Then
  \begin{align}\label{eq:newtonRatioFormula}
    d(v, \mu) = \ones - 2 r_s  + 2 e^{v} \circ A [W+A^{T}Q(v)A]^{-1} A^T e^{v} ( r_{\lambda} + r_{s} - \ones).
  \end{align}
  \fbox{How to show that $(d(v, \mu))_i < 1$}?
  \begin{proof}
     By definition, the Newton direction $d$ and associated $x$ satisfy
    \[
      d = \ones - \frac{1}{\sqrt\mu}  e^{v} \circ(A x + b),   \qquad (A^T Q(v) A +  W) x = 2 \sqrt\mu A^T e^v - (c + A^T Q(v) b).
    \]
    Letting $\Delta x = x - x_*$, we can rewrite $d$ in terms of $\Delta x$ as
    \begin{align}
      d &= \ones - \frac{1}{\sqrt\mu} e^{v} \circ ( A x_* + A \Delta x + b) \nonumber\\
        &= \ones - 2r_s - \frac{1}{\sqrt\mu} e^{v} \circ (  A \Delta x ) \label{eq:proofNewtonRatio}. 
    \end{align}
  Observing that
\[
  W x_* = -c + A^T \lambda_*, \qquad Q(v)s_* = e^v \circ (e^v \circ s_*) = 2 \sqrt\mu e^v \circ r_s,
\]
we also conclude that
    \begin{align*}
      (A^T Q(v) A +  W)\Delta x  &= 2 \sqrt\mu A^T e^v - (c + A^T Q(v) b) - (A^T Q(v) A +  W)x_*\\
                                 &= 2\sqrt \mu  A^{T} e^v   -A^T \lambda_*  -A^{T}Q(v)s_* \\
                                 &=  2\sqrt\mu  A^{T} e^v(\ones - r_{\lambda} - r_{s}),
    \end{align*}
    Solving for $\Delta x$ and substituting into~\eqref{eq:proofNewtonRatio} proves the claim.
\end{proof}
\end{lem}
\begin{cor}
  Suppose $W = 0$.
The Newton direction satisfies
  \[
    d(v, \mu) =  r_{\lambda} - r_{s} + 2 R (r_{\lambda} + r_{s} - \ones) 
  \]
where $R$ denotes the reflection operator for the subspace $\mathcal{L} = D \range A$.
  \begin{proof}
    By primal-dual symmetry
  \[
    d(v, \mu) = \ones - 2 r_s  + 2 P_{\mathcal{L}} ( r_{\lambda} + r_{s} - \ones), \qquad
    -d(v, \mu) = \ones - 2 r_{\lambda}  + 2 P_{\mathcal{L}^{\perp}} ( r_{\lambda} + r_{s} - \ones).
  \]
  \end{proof}
\end{cor}

\begin{lem}
  Suppose $W = 0$.
  If $r_{s} < 1 $ and $r_{\lambda} < 1$, then
  \[
    \|d(\mu(\alpha), v(\alpha) \| \le 1
  \]
  as $\alpha \rightarrow \infty$

  \begin{proof}

    Since $\lambda_*$ and $s_*$ are complementarity,
    we can keep the ratios the same by increasing $\alpha$
    and decreasing $\mu$.
Further, By Lemma 3.1 of Zhang, Bhatia, and Dennis, the projection
will converge to a coordinate projection onto the active set.
    Decomposing $\ones = \ones_{s} + \ones_{\lambda}$, where $\ones_s \circ r_s = r_s$
    and $\ones_{\lambda} \circ r_{\lambda} = r_{\lambda}$,
    we conclude that
  \[
    \lim_{\alpha \rightarrow \infty} d(v, \mu) = \ones - 2 r_s   + 2 ( r_{\lambda}  - \ones_{\lambda})
  \]
  which shows that
  \[
    \lim_{\alpha \rightarrow \infty}  d(v, \mu) = \ones_{s} - 2 r_s  +   2r_{\lambda} - \ones_{\lambda}.
  \]
  \end{proof}

\end{lem}



\begin{lem}
  Let $B^T = (b^T_1, b^T_2)$. How much does projection onto 
  the range of $B$ change the coordinate vector $e_1$?
  \[
    e^T_1 B(B^T B)^{-1} B^T e_1 = b^T_1 (\sum_i b^T_i b_i)^{-1} b_1
  \]
Let 
  \[
\trace b^T_1 b_1 (\sum_i b^T_i b_i)^{-1}
  \]
Let $U$ diagonalize $\sum_i b^T_i b_i$.  Then
Then
  \[
  \trace b_1  U \lambda^{-1} U^T b^T_1
  \]

\end{lem}

\begin{lem}
  At optimality, we assume that $G = W + A^T_1 D_1 A_1 \succ 0$.
  Note if this doesn't hold then
  \[
    W(x + \delta x) + (\lambda_* + D_1 A_1 \delta x) = c
  \]
  for any $\delta x$ in the kernel of $G$. By scaling up $D_1$, we
  can product arbitrarily large dual feasible $\lambda$.  This likely
  contradicts compactness.

\end{lem}
\[
  (B^T_1, B^T_2)^T  (B^T_1 B_1 + B^T_2 B_2)^{-1} (B^T_1 B^T_2)
\]
By the matrix inversion lemma,
\[
  (B^T_1 B_1 + B^T_2 B_2)^{-1} = (B^T_1  B_1)^{-1} - (B^T_1  B_1)^{-1}  B^T_2(I+ B_2(B_1^T B_1)^{-1} B^T_2)^{-1} B_2 (B^T_1 B_1)^{-1}
\]
Assume that $B_1$ is invertible and define $R =  B_2 B^{-1}_1$.
Then $RR^T = B_2 B^{-1}_1 B^{-T}_1 B^T_2 = B_2 (B^T_1 B_1)^{-1} B^T_2$


\begin{lem}
  Given $W$ and maps $B_1(\alpha)$ and $B_2(\alpha)$ 
  let $G(\alpha) = W + B^T_1 B_1$. 
  
And consider the map 
\[
  M(\alpha) = (B^T_1, B^T_2)^T (G + B_2^T B_2)^{-1} (B^T_1, B^T_2)
\]

Suppose that $G^{-1}$ exists for all $\alpha$
and that
  \begin{itemize}
    \item $\lim_{\alpha \rightarrow \infty} B_2 G^{-1}  = 0$
    \item $\lim_{\alpha \rightarrow \infty} B_2  = 0$
    \item $\lim_{\alpha \rightarrow \infty} B_2 G^{-1}B_1  = 0$
    \item $\lim_{\alpha \rightarrow \infty} B^T_1 B_1  = \infty$
  \end{itemize}
  Then $\lim_{\alpha \rightarrow \infty} M(\alpha) = P_{\range B_1} \oplus 0 $

  \begin{proof}
By the matrix inversion lemma:
    \[
  (G + B^T_2 B_2)^{-1} = G^{-1} - G^{-1}  B^T_2(I+ B_2G^{-1} B^T_2)^{-1} B_2 G^{-1}
\]
Hence, under assumptions
    \[
      \lim_{\alpha \rightarrow \infty}  (G + B^T_2 B_2)^{-1} - G^{-1}  = 0
    \]
    Defining
    \[
  \hat M(\alpha)  = (B^T_1, B^T_2)^T G^{-1} (B^T_1, B^T_2)
    \]
    We first claim that if $\lim_{\alpha} \hat M(\alpha)$ exists,
    then so does $\lim_{\alpha}  M(\alpha)$ and it is equal.

    To see this, let $ B = (B^T_1, B^T_2)$.
    \[
      \lim_{\alpha \rightarrow \infty} M(\alpha) = \hat M(\alpha) + \delta
    \]
    where
    \[
      \delta_{11} = B_1 G^{-1}  B^T_2(I+ B_2G^{-1} B^T_2)^{-1} B_2 G^{-1} B^T_1
    \]
    \[
      \delta_{12} = B_1 G^{-1}  B^T_2(I+ B_2G^{-1} B^T_2)^{-1} B_2 G^{-1} B^T_2
    \]
    \[
      \delta_{22} = B_2 G^{-1}  B^T_2(I+ B_2G^{-1} B^T_2)^{-1} B_2 G^{-1} B^T_2
    \]
    Under our assumptions, each term goes to zero and the claim holds.


    We have that $\hat M_{12}$ and $\hat M_{22}$
    equal zero since
\[
  \lim_{\alpha \rightarrow \infty}  = B_2 (G)^{-1} B^T_2 = 0, \qquad \lim_{\alpha \rightarrow \infty}  = B_1 (G)^{-1} B^T_2 = 0
\]

The remaining term is
\[
  \hat M_{11} = B_1 (W + B^T_1 B_1)^{-1} B^T_1 = P_{ \range B_1}
\]

  \end{proof}
\end{lem}


\begin{lem}
  Let $B_2(\alpha) = e^{-\alpha} B_2$.  Let $B_1(\alpha) = e^{\alpha} B_1$.
  \begin{itemize}
    \item $\lim_{\alpha \rightarrow \infty} B_2 G^{-1}  = 0$
    \item $\lim_{\alpha \rightarrow \infty} B_2  = 0$
    \item $\lim_{\alpha \rightarrow \infty} B_2 G^{-1}B_1  = 0$
    \item $\lim_{\alpha \rightarrow \infty} B^T_1 B_1  = \infty$
  \end{itemize}
  \[
    [B_2 G^{-1}](\alpha) =  e^{-\alpha} B_2(    W +  e^{2\alpha} B^T_1 B_1)^{-1} =  e^{-3\alpha} B_2( e^{-2\alpha}    W +   B^T_1 B_1)^{-1} 
  \]

  \[
    [B_2 G^{-1} B_1](\alpha) =    e^{-3\alpha} B_2( e^{-2\alpha}    W +   B^T_1 B_1)^{-1} e^{\alpha} B_1 =    e^{-2\alpha} B_2( e^{-2\alpha}    W +   B^T_1 B_1)^{-1} B_1
  \]
\end{lem}


\end{document} 





Using perspective sets and functions,
the t can be formulated as a mixed-integer convex optimization problem.
For this, we introduce variables  $(y_{e}, z_{e}) \in \mathbb{R}^n \times \mathbb{R}^n$
and $\varphi_e \in \mathbb{R}$ for each edge $e \in E$.
We let  $w_e$ denote the tuple $(y_e, z_e)$.
By abuse of notation, we let $X_e$ denote $X_{u} \times X_{v}$ for each edge $e = (u, v)$. 
Finally, we let  $\varphi \in \mathbb{R}^{|E|}$ denote the concatenation of $\varphi_e$
to state the problem as follows:
\begin{align*}
  \minimize_{w, \varphi}  &\sum_{e\in E}\tilde l_e(w_e, \varphi_e) \\
  \mbox{subject to }  
  & (w_e, \varphi_e) \in \tilde{ X_e},  \varphi_e \in \{0, 1\}  & \forall  e \in E \\
  & w_e = (y_e, z_e)\\
  &  (F^{in} - F^{out}) \varphi = g \\
  & F^{in}\varphi \le 1\\
  & \sum_{e \in E } F^{in}_{ve} y_{e} - \sum_{e \in E}  F^{out}_{ve}  z_{e} = 0,
  &  \forall  e \in V  \\
\end{align*}
where $F^{in} \in \mathbb{R}^{|V| \times |E|}$ and $F^{out} \in \mathbb{R}^{|V| \times |E|}$
are \emph{node-edge incidence matrices} for incoming and outgoing
edges, respectively,  and $g$ specifies source and sinks. 
Formally, $F^{in}_{ve} = 1$ if edge $e$ \emph{enters} vertex $v$, and is otherwise zero.
Similarly, $F^{out}_{ve} = 1$ if edge $e$ \emph{exits} vertex $v$ and is otherwise zero.
Finally, $g_v = 1$ if node $v$ is the source, $g_v = -1$ if $v$ is the sink, and $g_v = 0$ otherwise.


